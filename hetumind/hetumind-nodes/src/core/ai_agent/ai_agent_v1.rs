use std::sync::Arc;

use ahash::{HashMap, HashMapExt};
use async_trait::async_trait;
use hetumind_core::{
  types::JsonValue,
  version::Version,
  workflow::{
    ConnectionKind, EngineAction, EngineRequest, EngineResponse, ExecuteNodeAction, ExecutionData, ExecutionDataItems,
    ExecutionDataMap, InputPortConfig, NodeDefinition, NodeDefinitionBuilder, NodeExecutable, NodeExecutionContext,
    NodeExecutionError, NodeProperty, NodePropertyKind, OutputPortConfig, RegistrationError, make_execution_data_map,
  },
};
use serde_json::json;
use uuid::Uuid;

use crate::core::ai_agent::parameters::ToolExecutionStatus;

use super::parameters::{AiAgentConfig, ModelInstance, ToolCallRequest, ToolCallResult};

#[derive(Debug)]
pub struct AiAgentV1 {
  pub definition: Arc<NodeDefinition>,
}

impl AiAgentV1 {
  pub fn new() -> Result<Self, RegistrationError> {
    let base = NodeDefinitionBuilder::default();
    Self::try_from(base)
  }
}

impl TryFrom<NodeDefinitionBuilder> for AiAgentV1 {
  type Error = RegistrationError;

  fn try_from(mut base: NodeDefinitionBuilder) -> Result<Self, Self::Error> {
    base
      .kind(hetumind_core::workflow::NodeKind::from("ai_agent"))
      .version(hetumind_core::version::Version::new(1, 0, 0))
      .display_name("AI Agent")
      .description("AI Agent èŠ‚ç‚¹ï¼Œæ”¯æŒå·¥å…·è°ƒç”¨å’Œè®°å¿†åŠŸèƒ½")
      .icon("ğŸ¤–")

      // è¾“å…¥ç«¯å£
      .inputs([
        InputPortConfig::builder()
          .kind(ConnectionKind::Main)
          .display_name("Main Input")
          .required(true)
          .build(),
        InputPortConfig::builder()
          .kind(ConnectionKind::AiLanguageModel)
          .display_name("Large Language Model")
          .required(true)
          .max_connections(1)
          .build(),
        InputPortConfig::builder()
          .kind(ConnectionKind::AiMemory)
          .display_name("Memory(Vector storage)")
          .required(false)
          .build(),
        InputPortConfig::builder()
          .kind(ConnectionKind::AiTool)
          .display_name("AI Tool")
          .required(false)
          .build(),
      ])

      // è¾“å‡ºç«¯å£
      .outputs([
          OutputPortConfig::builder()
            .kind(ConnectionKind::Main)
            .display_name("AI å“åº”è¾“å‡º")
            .build(),
          OutputPortConfig::builder()
            .kind(ConnectionKind::AiTool)
            .display_name("å·¥å…·è°ƒç”¨è¯·æ±‚")
            .build(),
          OutputPortConfig::builder()
            .kind(ConnectionKind::Error)
            .display_name("é”™è¯¯è¾“å‡º")
            .build(),
      ])

      // å‚æ•°
      .properties([
        NodeProperty::builder()
          .display_name("ç³»ç»Ÿæç¤ºè¯")
          .name("system_prompt")
          .kind(NodePropertyKind::String)
          .required(false)
          .description("AI Agent çš„ç³»ç»Ÿæç¤ºè¯")
          .value(json!("ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„AIåŠ©æ‰‹"))
          .build(),
        NodeProperty::builder()
          .display_name("æœ€å¤§è¿­ä»£æ¬¡æ•°")
          .name("max_iterations")
          .kind(NodePropertyKind::Number)
          .required(false)
          .description("AI Agent çš„æœ€å¤§è¿­ä»£æ¬¡æ•°")
          .value(json!(10))
          .build(),
        NodeProperty::builder()
          .display_name("æ¸©åº¦å‚æ•°")
          .name("temperature")
          .kind(NodePropertyKind::Number)
          .required(false)
          .description("æ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„éšæœºæ€§")
          .value(json!(0.7))
          .build(),
        NodeProperty::builder()
          .display_name("æ˜¯å¦å¯ç”¨æµå¼å“åº”")
          .name("enable_streaming")
          .kind(NodePropertyKind::Boolean)
          .required(false)
          .description("æ˜¯å¦å¯ç”¨æµå¼å“åº”")
          .value(json!(false))
          .build(),
      ]);

    let definition = base.build()?;
    Ok(Self { definition: Arc::new(definition) })
  }
}

#[async_trait]
impl NodeExecutable for AiAgentV1 {
  async fn execute(
    &self,
    context: &NodeExecutionContext,
  ) -> Result<ExecutionDataMap, NodeExecutionError> {
    // 1. è·å–è¾“å…¥æ•°æ®å’Œé…ç½®
    let input_data = context.get_input_data("main")?;
    let config: AiAgentConfig = context.get_parameters()?;

    // 2. å¤„ç†å¼•æ“å“åº”ï¼ˆå·¥å…·è°ƒç”¨ç»“æœï¼‰
    if let Some(response) = &context.engine_response {
      return self.handle_tool_responses(context, response, &config).await;
    }

    // 3. è·å–è¿æ¥çš„ LLM å®ä¾‹
    let llm_instance = self.get_llm_instance(context).await?;

    // 4. è·å–è¿æ¥çš„å·¥å…·
    let tools = self.get_tools(context).await?;

    // 5. åˆ›å»º Agent
    let agent = self.create_agent(llm_instance, tools, &config).await?;

    // 6. æ‰§è¡Œ Agent
    let result = self.execute_agent(&agent, &input_data, &config).await?;

    // 7. è§£æå“åº”ï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦å·¥å…·è°ƒç”¨
    if let Some(tool_calls) = self.parse_tool_calls(&result) {
      // è¿”å›å¼•æ“è¯·æ±‚ä»¥æ‰§è¡Œå·¥å…·
      return self.create_engine_request(context, tool_calls, &config).await;
    }

    // 8. è¿”å›æœ€ç»ˆç»“æœ
    Ok(make_execution_data_map(vec![(
      ConnectionKind::Main,
      vec![ExecutionDataItems::Items(vec![ExecutionData::new_json(
        json!({
            "response": result,
            "agent_type": "ai_agent_v1",
            "timestamp": chrono::Utc::now().timestamp(),
        }),
        None,
      )])],
    )]))
  }

  fn definition(&self) -> Arc<NodeDefinition> {
    Arc::clone(&self.definition)
  }
}

impl AiAgentV1 {
  async fn get_llm_instance(&self, context: &NodeExecutionContext) -> Result<ModelInstance, NodeExecutionError> {
    // é€šè¿‡è¿æ¥ç±»å‹è·å– LLM å®ä¾‹
    let connection_data = context
      .get_connection_data(ConnectionKind::AiLanguageModel, 0)
      .ok_or_else(|| NodeExecutionError::ConnectionError("No LLM model connected".to_string()))?;

    // è§£æ LLM å®ä¾‹
    self.parse_llm_instance(connection_data)
  }

  fn parse_llm_instance(&self, connection_data: ExecutionData) -> Result<ModelInstance, NodeExecutionError> {
    let data = connection_data.json();
    serde_json::from_value(data.clone())
      .map_err(|e| NodeExecutionError::ConfigurationError(format!("Failed to parse LLM instance: {}", e)))
  }

  async fn get_tools(&self, context: &NodeExecutionContext) -> Result<Vec<JsonValue>, NodeExecutionError> {
    // è·å–æ‰€æœ‰è¿æ¥çš„å·¥å…·
    let tool_connections = context.get_all_connections(ConnectionKind::AiTool);

    let mut tools = Vec::new();
    for connection in tool_connections {
      tools.push(connection.json().clone());
    }

    Ok(tools)
  }

  async fn create_agent(
    &self,
    llm: ModelInstance,
    tools: Vec<JsonValue>,
    config: &AiAgentConfig,
  ) -> Result<JsonValue, NodeExecutionError> {
    // åˆ›å»º Agent é…ç½®
    Ok(json!({
        "llm": llm,
        "tools": tools,
        "system_prompt": config.system_prompt,
        "max_iterations": config.max_iterations,
        "temperature": config.temperature,
    }))
  }

  async fn execute_agent(
    &self,
    agent: &JsonValue,
    input_data: &ExecutionData,
    config: &AiAgentConfig,
  ) -> Result<String, NodeExecutionError> {
    // æ¨¡æ‹Ÿ Agent æ‰§è¡Œï¼Œå®é™…å®ç°éœ€è¦é›†æˆ rig-core
    let prompt = input_data.json().get("prompt").and_then(|v| v.as_str()).unwrap_or("è¯·å¤„ç†è¿™ä¸ªè¯·æ±‚");

    // è¿™é‡Œåº”è¯¥ä½¿ç”¨ rig-core çš„ Agent æ‰§è¡Œ
    // ç›®å‰è¿”å›æ¨¡æ‹Ÿå“åº”
    Ok(format!("AI Agent å“åº”: {}", prompt))
  }

  fn parse_tool_calls(&self, result: &str) -> Option<Vec<ToolCallRequest>> {
    // è§£æå·¥å…·è°ƒç”¨ï¼ˆè¿™é‡Œéœ€è¦å®ç°å®é™…çš„è§£æé€»è¾‘ï¼‰
    // ç›®å‰è¿”å›ç©ºåˆ—è¡¨ï¼Œè¡¨ç¤ºæ²¡æœ‰å·¥å…·è°ƒç”¨
    None
  }

  async fn handle_tool_responses(
    &self,
    context: &NodeExecutionContext,
    response: &EngineResponse,
    config: &AiAgentConfig,
  ) -> Result<ExecutionDataMap, NodeExecutionError> {
    // å¤„ç†å·¥å…·æ‰§è¡Œç»“æœï¼Œç»§ç»­å¯¹è¯
    let tool_results: Vec<ToolCallResult> =
      response.action_responses.iter().filter_map(|ar| self.extract_tool_result(ar)).collect();

    // æ„å»ºåŒ…å«å·¥å…·ç»“æœçš„æç¤º
    let prompt = self.build_prompt_with_tool_results(context, &tool_results, config).await?;

    // è·å– Agent å¹¶æ‰§è¡Œ
    let llm_instance = self.get_llm_instance(context).await?;
    let tools = self.get_tools(context).await?;
    let agent = self.create_agent(llm_instance, tools, config).await?;

    let input_data = ExecutionData::new_json(json!({"prompt": prompt}), None);
    let final_response = self.execute_agent(&agent, &input_data, config).await?;

    Ok(make_execution_data_map(vec![(
      ConnectionKind::Main,
      vec![ExecutionDataItems::Items(vec![ExecutionData::new_json(
        json!({
            "response": final_response,
            "tool_results": tool_results,
            "agent_type": "ai_agent_v1",
            "timestamp": chrono::Utc::now().timestamp(),
        }),
        None,
      )])],
    )]))
  }

  fn extract_tool_result(&self, result: &hetumind_core::workflow::EngineResult) -> Option<ToolCallResult> {
    // ä»å¼•æ“ç»“æœä¸­æå–å·¥å…·è°ƒç”¨ç»“æœ
    match &result.action {
      EngineAction::ExecuteNode(node_action) => {
        if let Some(data) = result.data.get(&ConnectionKind::AiTool) {
          if let Some(items) = data.first() {
            if let Some(data_items) = items.get_data_items() {
              if let Some(execution_data) = data_items.first() {
                return Some(ToolCallResult {
                  tool_call_id: node_action.action_id.to_string(),
                  tool_name: node_action.node_name.clone(),
                  result: execution_data.json().clone(),
                  status: ToolExecutionStatus::Success,
                });
              }
            }
          }
        }
      }
      _ => {}
    }
    None
  }

  async fn build_prompt_with_tool_results(
    &self,
    _context: &NodeExecutionContext,
    tool_results: &Vec<ToolCallResult>,
    _config: &AiAgentConfig,
  ) -> Result<String, NodeExecutionError> {
    // æ„å»ºåŒ…å«å·¥å…·ç»“æœçš„æç¤º
    let mut prompt = String::new();
    for result in tool_results {
      prompt.push_str(&format!("å·¥å…· {} æ‰§è¡Œç»“æœ: {:?}\n", result.tool_name, result.result));
    }
    Ok(prompt)
  }

  async fn create_engine_request(
    &self,
    _context: &NodeExecutionContext,
    tool_calls: Vec<ToolCallRequest>,
    config: &AiAgentConfig,
  ) -> Result<ExecutionDataMap, NodeExecutionError> {
    let actions: Vec<EngineAction> = tool_calls
      .into_iter()
      .map(|tool_call| {
        let tool_name = tool_call.tool_name.clone();
        EngineAction::ExecuteNode(ExecuteNodeAction {
          node_name: tool_call.tool_name,
          input: tool_call.parameters,
          connection_type: ConnectionKind::AiTool,
          action_id: Uuid::new_v4(),
          metadata: {
            let mut meta = HashMap::new();
            meta.insert("tool_call_id".to_string(), json!(tool_call.id));
            meta.insert("tool_name".to_string(), json!(tool_name));
            meta
          },
        })
      })
      .collect();

    let engine_request = EngineRequest {
      actions,
      metadata: {
        let mut meta = HashMap::new();
        meta.insert("request_type".to_string(), json!("tool_execution"));
        meta.insert("config".to_string(), json!(config));
        meta
      },
      request_id: Uuid::new_v4(),
    };

    Ok(make_execution_data_map(vec![(
      ConnectionKind::AiTool,
      vec![ExecutionDataItems::Items(vec![ExecutionData::new_json(json!(engine_request), None)])],
    )]))
  }
}
