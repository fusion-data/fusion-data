use std::sync::Arc;

use ahash::{HashMap, HashMapExt};
use async_trait::async_trait;
use hetumind_core::{
  types::JsonValue,
  version::Version,
  workflow::{
    ConnectionKind, EngineAction, EngineRequest, EngineResponse, ExecuteNodeAction, ExecutionData, ExecutionDataItems,
    ExecutionDataMap, InputPortConfig, NodeDefinition, NodeExecutable, NodeExecutionContext, NodeExecutionError,
    NodeProperty, NodePropertyKind, OutputPortConfig, RegistrationError, make_execution_data_map,
  },
};
use rig::{client::CompletionClient, completion::Prompt};
use serde_json::json;
use uuid::Uuid;

use crate::core::ai_agent::tool_manager::ToolManager;
use crate::core::connection_manager::OptimizedConnectionContext;
use crate::{constants::AI_AGENT_NODE_KIND, core::ai_agent::parameters::ToolExecutionStatus};

use super::parameters::{AiAgentConfig, ModelInstance, ToolCallRequest, ToolCallResult};

#[allow(dead_code)]
pub struct AiAgentV1 {
  pub definition: Arc<NodeDefinition>,
  #[allow(dead_code)]
  tool_manager: Arc<tokio::sync::RwLock<ToolManager>>,
}

impl AiAgentV1 {
  pub fn new() -> Result<Self, RegistrationError> {
    let base = NodeDefinition::new(AI_AGENT_NODE_KIND, "AI Agent");
    Self::try_from(base)
  }
}

impl TryFrom<NodeDefinition> for AiAgentV1 {
  type Error = RegistrationError;

  fn try_from(base: NodeDefinition) -> Result<Self, Self::Error> {
    let definition = base
      .with_version(Version::new(1, 0, 0))
      .with_description("AI Agent èŠ‚ç‚¹ï¼Œæ”¯æŒå·¥å…·è°ƒç”¨å’Œè®°å¿†åŠŸèƒ½")
      .with_icon("ğŸ¤–")
      // è¾“å…¥ç«¯å£
      .add_input(InputPortConfig::new(ConnectionKind::Main, "Main Input")
          .with_required(true))
      .add_input(InputPortConfig::new(ConnectionKind::AiModel, "Large Language Model")
          .with_required(true)
          .with_max_connections(1))
      .add_input(InputPortConfig::new(ConnectionKind::AiMemory, "Memory(Vector storage)")
          .with_required(false))
      .add_input(InputPortConfig::new(ConnectionKind::AiTool, "AI Tool")
          .with_required(false))

      // è¾“å‡ºç«¯å£
      .add_output(OutputPortConfig::new(ConnectionKind::Main, "AI å“åº”è¾“å‡º"))
      .add_output(OutputPortConfig::new(ConnectionKind::AiTool, "å·¥å…·è°ƒç”¨è¯·æ±‚"))
      .add_output(OutputPortConfig::new(ConnectionKind::Error, "é”™è¯¯è¾“å‡º"))

      // å‚æ•°
      .add_property(NodeProperty::new(NodePropertyKind::String)
          .with_display_name("ç³»ç»Ÿæç¤ºè¯")
          .with_name("system_prompt")
          .with_required(false)
          .with_description("AI Agent çš„ç³»ç»Ÿæç¤ºè¯")
          .with_value(json!("ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„AIåŠ©æ‰‹")))
      .add_property(NodeProperty::new(NodePropertyKind::Number)
          .with_display_name("æœ€å¤§è¿­ä»£æ¬¡æ•°")
          .with_name("max_iterations")
          .with_required(false)
          .with_description("AI Agent çš„æœ€å¤§è¿­ä»£æ¬¡æ•°")
          .with_value(json!(10)))
      .add_property(NodeProperty::new(NodePropertyKind::Number)
          .with_display_name("æ¸©åº¦å‚æ•°")
          .with_name("temperature")
          .with_required(false)
          .with_description("æ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„éšæœºæ€§")
          .with_value(json!(0.7)))
      .add_property(NodeProperty::new(NodePropertyKind::Boolean)
          .with_display_name("æ˜¯å¦å¯ç”¨æµå¼å“åº”")
          .with_name("enable_streaming")
          .with_required(false)
          .with_description("æ˜¯å¦å¯ç”¨æµå¼å“åº”")
          .with_value(json!(false)));

    Ok(Self { definition: Arc::new(definition), tool_manager: Arc::new(tokio::sync::RwLock::new(ToolManager::new())) })
  }
}

#[async_trait]
impl NodeExecutable for AiAgentV1 {
  async fn execute(&self, context: &NodeExecutionContext) -> Result<ExecutionDataMap, NodeExecutionError> {
    // 1. è·å–è¾“å…¥æ•°æ®å’Œé…ç½®
    let input_data = context.get_input_data("main")?;
    let config: AiAgentConfig = context.get_parameters()?;

    // 2. å¤„ç†å¼•æ“å“åº”ï¼ˆå·¥å…·è°ƒç”¨ç»“æœï¼‰
    if let Some(response) = &context.engine_response {
      return self.handle_tool_responses(context, response, &config).await;
    }

    // 3. è·å–è¿æ¥çš„ LLM å®ä¾‹
    let llm_instance = self.get_llm_instance(context).await?;

    // 4. è·å–è¿æ¥çš„å·¥å…·
    let tools = self.get_tools(context).await?;

    // 5. åˆ›å»º Agent
    let agent = self.create_agent(llm_instance, tools, &config).await?;

    // 6. æ‰§è¡Œ Agent
    let result = if config.enable_streaming {
      self.execute_agent_streaming(&agent, &input_data, &config).await?
    } else {
      self.execute_agent(&agent, &input_data, &config).await?
    };

    // 7. è§£æå“åº”ï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦å·¥å…·è°ƒç”¨
    if let Some(tool_calls) = self.parse_tool_calls(&result) {
      // è¿”å›å¼•æ“è¯·æ±‚ä»¥æ‰§è¡Œå·¥å…·
      return self.create_engine_request(context, tool_calls, &config).await;
    }

    // 8. è¿”å›æœ€ç»ˆç»“æœ
    Ok(make_execution_data_map(vec![(
      ConnectionKind::Main,
      vec![ExecutionDataItems::Items(vec![ExecutionData::new_json(
        json!({
            "response": result,
            "agent_type": "ai_agent_v1",
            "streaming": config.enable_streaming,
            "timestamp": chrono::Utc::now().timestamp(),
        }),
        None,
      )])],
    )]))
  }

  fn definition(&self) -> Arc<NodeDefinition> {
    Arc::clone(&self.definition)
  }
}

impl AiAgentV1 {
  async fn get_llm_instance(&self, context: &NodeExecutionContext) -> Result<ModelInstance, NodeExecutionError> {
    // é€šè¿‡ä¼˜åŒ–çš„è¿æ¥ç±»å‹è·å– LLM å®ä¾‹
    let connection_data = context
      .get_connection_data_optimized(ConnectionKind::AiModel, 0)
      .await?
      .ok_or_else(|| NodeExecutionError::ConnectionError("No LLM model connected".to_string()))?;

    // è§£æ LLM å®ä¾‹
    self.parse_llm_instance(connection_data)
  }

  fn parse_llm_instance(&self, connection_data: ExecutionData) -> Result<ModelInstance, NodeExecutionError> {
    let data = connection_data.json();
    serde_json::from_value(data.clone())
      .map_err(|e| NodeExecutionError::ConfigurationError(format!("Failed to parse LLM instance: {}", e)))
  }

  async fn get_tools(&self, context: &NodeExecutionContext) -> Result<Vec<JsonValue>, NodeExecutionError> {
    // è·å–æ‰€æœ‰è¿æ¥çš„å·¥å…·ï¼ˆä½¿ç”¨ä¼˜åŒ–çš„æ‰¹é‡è·å–ï¼‰
    let tool_connections = context.get_all_connections_optimized(ConnectionKind::AiTool).await?;

    let mut tools = Vec::new();
    for connection in tool_connections {
      tools.push(connection.json().clone());
    }

    Ok(tools)
  }

  #[allow(unused_variables)]
  async fn create_agent(
    &self,
    llm_instance: ModelInstance,
    _tools: Vec<JsonValue>,
    config: &AiAgentConfig,
  ) -> Result<Box<dyn std::any::Any + Send + Sync>, NodeExecutionError> {
    // åˆ›å»º rig-core Agent
    let agent = self.create_rig_agent(&llm_instance).await?;
    Ok(Box::new(agent))
  }

  async fn execute_agent(
    &self,
    agent: &Box<dyn std::any::Any + Send + Sync>,
    input_data: &ExecutionData,
    _config: &AiAgentConfig,
  ) -> Result<String, NodeExecutionError> {
    let prompt = input_data
      .json()
      .get("prompt")
      .and_then(|v| v.as_str())
      .map(|s| s.to_string())
      .unwrap_or_else(|| input_data.json().to_string());

    // å°è¯•è½¬æ¢ä¸ºä¸åŒç±»å‹çš„ Agent
    if let Some(openai_agent) =
      agent.downcast_ref::<rig::agent::Agent<rig::providers::openai::completion::CompletionModel>>()
    {
      openai_agent.prompt(prompt).await.map_err(|e| NodeExecutionError::ExecutionFailed {
        node_name: "AiAgentV1".to_string().into(),
        message: Some(format!("OpenAI API call failed: {}", e)),
      })
    } else if let Some(anthropic_agent) =
      agent.downcast_ref::<rig::agent::Agent<rig::providers::anthropic::completion::CompletionModel>>()
    {
      anthropic_agent.prompt(prompt).await.map_err(|_| NodeExecutionError::ExecutionFailed {
        node_name: "AiAgentV1".to_string().into(),
        message: Some("Anthropic API call failed".to_string()),
      })
    } else {
      Err(NodeExecutionError::ExecutionFailed {
        node_name: "AiAgentV1".to_string().into(),
        message: Some("Unsupported AI Agent type".to_string()),
      })
    }
  }

  #[allow(unused_variables)]
  fn parse_tool_calls(&self, result: &str) -> Option<Vec<ToolCallRequest>> {
    // è§£æå·¥å…·è°ƒç”¨ï¼ˆè¿™é‡Œéœ€è¦å®ç°å®é™…çš„è§£æé€»è¾‘ï¼‰
    // ç›®å‰è¿”å›ç©ºåˆ—è¡¨ï¼Œè¡¨ç¤ºæ²¡æœ‰å·¥å…·è°ƒç”¨
    None
  }

  async fn handle_tool_responses(
    &self,
    context: &NodeExecutionContext,
    response: &EngineResponse,
    config: &AiAgentConfig,
  ) -> Result<ExecutionDataMap, NodeExecutionError> {
    // å¤„ç†å·¥å…·æ‰§è¡Œç»“æœï¼Œç»§ç»­å¯¹è¯
    let tool_results: Vec<ToolCallResult> =
      response.action_responses.iter().filter_map(|ar| self.extract_tool_result(ar)).collect();

    // æ„å»ºåŒ…å«å·¥å…·ç»“æœçš„æç¤º
    let prompt = self.build_prompt_with_tool_results(context, &tool_results, config).await?;

    // è·å– Agent å¹¶æ‰§è¡Œ
    let llm_instance = self.get_llm_instance(context).await?;
    let tools = self.get_tools(context).await?;
    let agent = self.create_agent(llm_instance, tools, config).await?;

    let input_data = ExecutionData::new_json(json!({"prompt": prompt}), None);
    let final_response = self.execute_agent(&agent, &input_data, config).await?;

    Ok(make_execution_data_map(vec![(
      ConnectionKind::Main,
      vec![ExecutionDataItems::Items(vec![ExecutionData::new_json(
        json!({
            "response": final_response,
            "tool_results": tool_results,
            "agent_type": "ai_agent_v1",
            "timestamp": chrono::Utc::now().timestamp(),
        }),
        None,
      )])],
    )]))
  }

  fn extract_tool_result(&self, result: &hetumind_core::workflow::EngineResult) -> Option<ToolCallResult> {
    // ä»å¼•æ“ç»“æœä¸­æå–å·¥å…·è°ƒç”¨ç»“æœ
    match &result.action {
      EngineAction::ExecuteNode(node_action) => {
        if let Some(data) = result.data.get(&ConnectionKind::AiTool)
          && let Some(items) = data.first()
          && let Some(data_items) = items.get_data_items()
          && let Some(execution_data) = data_items.first()
        {
          return Some(ToolCallResult {
            tool_call_id: node_action.action_id.to_string(),
            tool_name: node_action.node_name.clone(),
            result: execution_data.json().clone(),
            status: ToolExecutionStatus::Success,
          });
        }
      }
      _ => {}
    }
    None
  }

  async fn build_prompt_with_tool_results(
    &self,
    _context: &NodeExecutionContext,
    tool_results: &Vec<ToolCallResult>,
    _config: &AiAgentConfig,
  ) -> Result<String, NodeExecutionError> {
    // æ„å»ºåŒ…å«å·¥å…·ç»“æœçš„æç¤º
    let mut prompt = String::new();
    for result in tool_results {
      prompt.push_str(&format!("å·¥å…· {} æ‰§è¡Œç»“æœ: {:?}\n", result.tool_name, result.result));
    }
    Ok(prompt)
  }

  async fn create_engine_request(
    &self,
    _context: &NodeExecutionContext,
    tool_calls: Vec<ToolCallRequest>,
    config: &AiAgentConfig,
  ) -> Result<ExecutionDataMap, NodeExecutionError> {
    let actions: Vec<EngineAction> = tool_calls
      .into_iter()
      .map(|tool_call| {
        let tool_name = tool_call.tool_name.clone();
        EngineAction::ExecuteNode(ExecuteNodeAction {
          node_name: tool_call.tool_name,
          input: tool_call.parameters,
          connection_type: ConnectionKind::AiTool,
          action_id: Uuid::new_v4(),
          metadata: {
            let mut meta = HashMap::new();
            meta.insert("tool_call_id".to_string(), json!(tool_call.id));
            meta.insert("tool_name".to_string(), json!(tool_name));
            meta
          },
        })
      })
      .collect();

    let engine_request = EngineRequest {
      actions,
      metadata: {
        let mut meta = HashMap::new();
        meta.insert("request_type".to_string(), json!("tool_execution"));
        meta.insert("config".to_string(), json!(config));
        meta
      },
      request_id: Uuid::new_v4(),
    };

    Ok(make_execution_data_map(vec![(
      ConnectionKind::AiTool,
      vec![ExecutionDataItems::Items(vec![ExecutionData::new_json(json!(engine_request), None)])],
    )]))
  }

  /// åˆ›å»º rig-core Agent å®ä¾‹
  async fn create_rig_agent(
    &self,
    llm_instance: &ModelInstance,
  ) -> Result<Box<dyn std::any::Any + Send + Sync>, NodeExecutionError> {
    // ä»ModelInstanceçš„configä¸­è·å–LLMé…ç½®
    let config = &llm_instance.config;

    let provider = config.get("provider").and_then(|v| v.as_str()).unwrap_or("openai");

    match provider {
      "openai" => {
        use rig::providers::openai;

        let api_key = config
          .get("api_key")
          .and_then(|v| v.as_str())
          .ok_or_else(|| NodeExecutionError::ConfigurationError("OpenAI API key not found".to_string()))?;

        let client = openai::Client::new(api_key);

        // æ ¹æ®æ–‡æ¡£ç¤ºä¾‹ï¼Œç›´æ¥ä½¿ç”¨ client.agent() æ–¹æ³•åˆ›å»º
        let model_name = config.get("model").and_then(|v| v.as_str()).unwrap_or("gpt-3.5-turbo");

        let agent = client.agent(model_name);
        Ok(Box::new(agent))
      }
      "anthropic" => {
        use rig::providers::anthropic;

        let api_key = config
          .get("api_key")
          .and_then(|v| v.as_str())
          .ok_or_else(|| NodeExecutionError::ConfigurationError("Anthropic API key not found".to_string()))?;

        let client = anthropic::Client::new(api_key);

        let model_name = config.get("model").and_then(|v| v.as_str()).unwrap_or("claude-3-sonnet-20240229");

        let agent = client.agent(model_name);
        Ok(Box::new(agent))
      }
      _ => Err(NodeExecutionError::ConfigurationError(format!("Unsupported LLM provider: {}", provider))),
    }
  }

  /// å°†å·¥å…·å®šä¹‰è½¬æ¢ä¸º rig-core æ ¼å¼ï¼ˆæš‚æ—¶ä¸å®ç°ï¼‰
  #[allow(dead_code)]
  async fn convert_to_rig_tool(&self, _tool: JsonValue) -> Result<String, NodeExecutionError> {
    // TODO: æš‚æ—¶ä¸å®ç°å·¥å…·è½¬æ¢ï¼Œç›´æ¥è¿”å›å·¥å…·åç§°
    Ok("tool_conversion_not_implemented".to_string())
  }

  /// æµå¼æ‰§è¡ŒAgent
  #[allow(unused_variables)]
  async fn execute_agent_streaming(
    &self,
    agent: &Box<dyn std::any::Any + Send + Sync>,
    input_data: &ExecutionData,
    config: &AiAgentConfig,
  ) -> Result<String, NodeExecutionError> {
    let prompt = input_data
      .json()
      .get("prompt")
      .and_then(|v| v.as_str())
      .map(|s| s.to_string())
      .unwrap_or_else(|| input_data.json().to_string());

    // æ¨¡æ‹Ÿæµå¼Agentæ‰§è¡Œ
    let mut streaming_response = String::new();

    // æ¨¡æ‹ŸAI Agentæ€è€ƒè¿‡ç¨‹çš„æµå¼è¾“å‡º
    let thinking_steps = vec!["æ­£åœ¨åˆ†æç”¨æˆ·è¯·æ±‚...", "ç†è§£é—®é¢˜ä¸Šä¸‹æ–‡...", "å‡†å¤‡ç”Ÿæˆå“åº”..."];

    for step in thinking_steps {
      streaming_response.push_str(&format!("[{}]\n", step));

      // æ¨¡æ‹Ÿæ€è€ƒæ—¶é—´
      tokio::time::sleep(tokio::time::Duration::from_millis(200)).await;
    }

    // æ‰§è¡Œå®é™…çš„Agentæ¨ç†
    let final_response = self.execute_agent(agent, input_data, config).await?;
    streaming_response.push_str(&final_response);

    Ok(streaming_response)
  }
}
