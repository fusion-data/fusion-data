use std::sync::Arc;

use async_trait::async_trait;
use hetumind_core::{
  types::JsonValue,
  version::Version,
  workflow::{
    ConnectionKind, ExecutionData, ExecutionDataItems, ExecutionDataMap, InputPortConfig, NodeDefinition,
    NodeExecutable, NodeExecutionContext, NodeExecutionError, NodeGroupKind, NodeProperty, NodePropertyKind,
    OutputPortConfig, RegistrationError,
  },
};
use rig::providers::{anthropic::Client as AnthropicClient, openai::Client as OpenAIClient};
use serde_json::json;

use crate::constants::CHAT_MODEL_NODE_KIND;

use super::parameters::{
  AnthropicModel, LlmConfig, LocalModel, ModelCapabilities, ModelClient, OpenAIModel, StreamingResponse, UsageStats,
};

#[derive(Debug)]
pub struct ChatModelV1 {
  pub definition: Arc<NodeDefinition>,
}

impl ChatModelV1 {
  pub fn new() -> Result<Self, RegistrationError> {
    let base = NodeDefinition::new(CHAT_MODEL_NODE_KIND, "LLM Chat Model");
    Self::try_from(base)
  }
}

impl TryFrom<NodeDefinition> for ChatModelV1 {
  type Error = RegistrationError;

  fn try_from(base: NodeDefinition) -> Result<Self, Self::Error> {
    let definition = base
      .with_version(Version::new(1, 0, 0))
      .with_description("LLM èŠå¤©æ¨¡å‹èŠ‚ç‚¹ï¼Œæ”¯æŒå¤šç§æ¨¡å‹æä¾›è€…")
      .add_group(NodeGroupKind::Transform)
      .with_icon("ğŸ§ ")
      // è¾“å…¥ç«¯å£
      .add_input(InputPortConfig::builder()
          .kind(ConnectionKind::Main)
          .display_name("èŠå¤©æ¶ˆæ¯è¾“å…¥")
          .required(true)
          .build())
      // è¾“å‡ºç«¯å£
      .add_output(OutputPortConfig::builder()
            .kind(ConnectionKind::Main)
            .display_name("æ¨¡å‹å“åº”")
            .build())
      // æ¨¡å‹å®ä¾‹ç«¯å£
      .add_output(OutputPortConfig::builder()
            .kind(ConnectionKind::AiModel)
            .display_name("æ¨¡å‹å®ä¾‹")
            .build())
      // é”™è¯¯è¾“å‡ºç«¯å£
      .add_output(OutputPortConfig::builder()
            .kind(ConnectionKind::Error)
            .display_name("é”™è¯¯è¾“å‡º")
            .build())
      // å‚æ•°
      .add_property(NodeProperty::builder()
            .name("provider")
            .kind(NodePropertyKind::String)
            .display_name("æ¨¡å‹æä¾›è€…")
            .value(json!("openai"))
            .required(true)
            .build())
      .add_property(NodeProperty::builder()
            .name("model")
            .kind(NodePropertyKind::String)
            .display_name("æ¨¡å‹åç§°")
            .value(json!("gpt-3.5-turbo"))
            .required(true).build())
          .add_property(NodeProperty::builder()
            .name("credential_id")
            .kind(NodePropertyKind::String)
            .display_name("å‡­è¯ID")
            .description("ç”¨äºè·å–APIå¯†é’¥çš„å‡­è¯IDï¼Œå¦‚æœæä¾›åˆ™ä¼˜å…ˆä½¿ç”¨å‡­è¯æœåŠ¡")
            .required(false).build())
          .add_property(NodeProperty::builder()
            .name("api_key")
            .kind(NodePropertyKind::String)
            .display_name("API å¯†é’¥")
            .description("APIå¯†é’¥ï¼Œå½“æœªæŒ‡å®šå‡­è¯IDæ—¶ä½¿ç”¨")
            .required(false).build())
          .add_property(NodeProperty::builder()
            .name("base_url")
            .kind(NodePropertyKind::String)
            .display_name("API åŸºç¡€URL")
            .required(false).build())
          .add_property(NodeProperty::builder()
            .name("max_tokens")
            .kind(NodePropertyKind::Number)
            .display_name("æœ€å¤§ä»¤ç‰Œæ•°")
            .value(json!(2000))
            .required(false).build())
          .add_property(NodeProperty::builder()
            .name("temperature")
            .kind(NodePropertyKind::Number)
            .display_name("æ¸©åº¦å‚æ•°")
            .value(json!(0.7))
            .required(false).build())
          .add_property(NodeProperty::builder()
            .name("stream")
            .kind(NodePropertyKind::Boolean)
            .display_name("æ˜¯å¦å¯ç”¨æµå¼å“åº”")
            .value(json!(false))
            .required(false).build())
          .add_property(NodeProperty::builder()
            .name("timeout")
            .kind(NodePropertyKind::Number)
            .display_name("è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰")
            .value(json!(60))
            .required(false)
            .build());
    Ok(Self { definition: Arc::new(definition) })
  }
}

#[async_trait]
impl NodeExecutable for ChatModelV1 {
  async fn execute(&self, context: &NodeExecutionContext) -> Result<ExecutionDataMap, NodeExecutionError> {
    // 1. è·å–è¾“å…¥æ•°æ®å’Œé…ç½®
    let input_data = context.get_input_data("main")?;
    let config: LlmConfig = context.get_parameters()?;

    // 2. åˆ›å»ºæ¨¡å‹å®¢æˆ·ç«¯
    let model_client = self.create_model_client(&config).await?;

    // 3. æ‰§è¡Œæ¨ç†
    let response = if config.stream {
      self.execute_streaming_inference(&model_client, &input_data, &config).await?
    } else {
      self.execute_standard_inference(&model_client, &input_data, &config).await?
    };

    // 4. æ„å»ºè¾“å‡ºæ•°æ®
    Ok(make_execution_data_map(vec![
      ("main", ExecutionDataItems::Items(vec![ExecutionData::new_json(response.clone(), None)])),
      (
        "ai_model",
        ExecutionDataItems::Items(vec![ExecutionData::new_json(
          json!({
              "client": self.serialize_model_client(&model_client),
              "config": json!(config),
              "capabilities": self.get_model_capabilities(&config),
              "model_id": uuid::Uuid::new_v4().to_string(),
          }),
          None,
        )]),
      ),
    ]))
  }

  fn definition(&self) -> Arc<NodeDefinition> {
    Arc::clone(&self.definition)
  }
}

impl ChatModelV1 {
  async fn create_model_client(&self, config: &LlmConfig) -> Result<ModelClient, NodeExecutionError> {
    match config.provider.as_str() {
      "openai" => {
        let api_key = self.get_api_key_from_config_or_env(config, "OPENAI_API_KEY").await?;
        let client = OpenAIClient::new(&api_key);

        // åˆ›å»ºä¸€ä¸ªç®€åŒ–çš„ModelClientï¼Œä¸ä½¿ç”¨rig-coreçš„modelæ–¹æ³•
        Ok(ModelClient::OpenAI(OpenAIModel { model: config.model.clone(), api_key, base_url: config.base_url.clone() }))
      }
      "anthropic" => {
        let api_key = self.get_api_key_from_config_or_env(config, "ANTHROPIC_API_KEY").await?;
        let client = AnthropicClient::new(&api_key);

        // åˆ›å»ºä¸€ä¸ªç®€åŒ–çš„ModelClient
        Ok(ModelClient::Anthropic(AnthropicModel {
          model: config.model.clone(),
          api_key,
          base_url: config.base_url.clone(),
        }))
      }
      "local" => {
        let endpoint = config
          .base_url
          .as_ref()
          .ok_or_else(|| NodeExecutionError::ConfigurationError("Local model endpoint not provided".to_string()))?;

        Ok(ModelClient::Local(LocalModel {
          endpoint: endpoint.clone(),
          model_name: config.model.clone(),
          api_key: config.api_key.clone(),
        }))
      }
      provider => Err(NodeExecutionError::ConfigurationError(format!("Unsupported provider: {}", provider))),
    }
  }

  /// ä»é…ç½®æˆ–ç¯å¢ƒå˜é‡è·å–APIå¯†é’¥ï¼Œæ”¯æŒå‡­è¯æœåŠ¡
  async fn get_api_key_from_config_or_env(
    &self,
    config: &LlmConfig,
    env_var: &str,
  ) -> Result<String, NodeExecutionError> {
    // 1. ä¼˜å…ˆä½¿ç”¨é…ç½®ä¸­çš„APIå¯†é’¥
    if let Some(api_key) = &config.api_key {
      return Ok(api_key.clone());
    }

    // 2. å¦‚æœé…ç½®äº†å‡­è¯IDï¼Œå°è¯•ä»å‡­è¯æœåŠ¡è·å–ï¼ˆè¿™é‡Œéœ€è¦æ³¨å…¥å‡­è¯æœåŠ¡ï¼‰
    if let Some(credential_id) = &config.credential_id {
      // TODO: è¿™é‡Œéœ€è¦å®é™…çš„å‡­è¯æœåŠ¡é›†æˆ
      // ç›®å‰è¿”å›é”™è¯¯ï¼Œæç¤ºéœ€è¦å®ç°å‡­è¯æœåŠ¡é›†æˆ
      return Err(NodeExecutionError::ConfigurationError(format!(
        "Credential service integration not yet implemented for credential_id: {}",
        credential_id
      )));
    }

    // 3. æœ€åå°è¯•ä»ç¯å¢ƒå˜é‡è·å–
    std::env::var(env_var).map_err(|_| {
      NodeExecutionError::ConfigurationError(format!(
        "API key not provided. Please set credential_id, api_key parameter, or {} environment variable",
        env_var
      ))
    })
  }

  async fn execute_standard_inference(
    &self,
    client: &ModelClient,
    input_data: &hetumind_core::workflow::ExecutionData,
    config: &LlmConfig,
  ) -> Result<JsonValue, NodeExecutionError> {
    let prompt = input_data
      .json()
      .get("prompt")
      .and_then(|v| v.as_str())
      .ok_or_else(|| NodeExecutionError::InvalidInput("No prompt provided".to_string()))?;

    // æ¨¡æ‹Ÿæ¨ç†è¿‡ç¨‹ï¼Œå®é™…å®ç°éœ€è¦é›†æˆå…·ä½“çš„LLMå®¢æˆ·ç«¯
    let mock_response = format!("è¿™æ˜¯æ¥è‡ª {} æ¨¡å‹çš„å“åº”: {}", config.model, prompt);

    let usage = UsageStats {
      prompt_tokens: (prompt.len() as f32 / 4.0) as u32, // ä¼°ç®—tokenæ•°
      completion_tokens: (mock_response.len() as f32 / 4.0) as u32,
      total_tokens: 0,
      estimated_cost: 0.0,
    };

    let total_tokens = usage.prompt_tokens + usage.completion_tokens;

    Ok(json!({
        "response": mock_response,
        "model": config.model,
        "provider": config.provider,
        "usage": usage,
        "timestamp": chrono::Utc::now().timestamp(),
        "request_id": uuid::Uuid::new_v4().to_string(),
    }))
  }

  async fn execute_streaming_inference(
    &self,
    client: &ModelClient,
    input_data: &hetumind_core::workflow::ExecutionData,
    config: &LlmConfig,
  ) -> Result<JsonValue, NodeExecutionError> {
    let prompt = input_data
      .json()
      .get("prompt")
      .and_then(|v| v.as_str())
      .ok_or_else(|| NodeExecutionError::InvalidInput("No prompt provided".to_string()))?;

    // åˆ›å»ºæµå¼å“åº”
    let stream_id = uuid::Uuid::new_v4().to_string();
    let mut streaming_response =
      StreamingResponse::new(stream_id.clone(), config.model.clone(), config.provider.clone());

    // æ¨¡æ‹Ÿæµå¼å“åº”ç”Ÿæˆè¿‡ç¨‹
    let simulated_chunks = self.simulate_streaming_chunks(&config.model, prompt).await?;

    // æ·»åŠ æµå¼å—
    for chunk in simulated_chunks {
      streaming_response.add_chunk(chunk);
    }

    // å®Œæˆæµå¼å“åº”
    streaming_response.finish();

    Ok(json!({
        "streaming_response": streaming_response,
        "model": config.model,
        "provider": config.provider,
        "streaming": true,
        "timestamp": chrono::Utc::now().timestamp(),
        "request_id": streaming_response.metadata.request_id,
    }))
  }

  /// æ¨¡æ‹Ÿæµå¼å“åº”å—ç”Ÿæˆ
  async fn simulate_streaming_chunks(&self, model: &str, prompt: &str) -> Result<Vec<String>, NodeExecutionError> {
    let full_response = format!("è¿™æ˜¯æ¥è‡ª {} æ¨¡å‹çš„æµå¼å“åº”: {}", model, prompt);

    // å°†å®Œæ•´å“åº”åˆ†å‰²æˆå¤šä¸ªå—æ¥æ¨¡æ‹Ÿæµå¼è¾“å‡º
    let chunk_size = 10; // æ¯ä¸ªå—10ä¸ªå­—ç¬¦
    let mut chunks = Vec::new();

    for (i, chunk) in full_response.chars().collect::<Vec<_>>().chunks(chunk_size).enumerate() {
      let chunk_str: String = chunk.iter().collect();

      // æ¨¡æ‹Ÿç½‘ç»œå»¶è¿Ÿ
      tokio::time::sleep(tokio::time::Duration::from_millis(100)).await;

      chunks.push(chunk_str);

      // æœ€åä¸€ä¸ªå—æ·»åŠ å®Œæˆæ ‡è®°
      if i == (full_response.chars().count() + chunk_size - 1) / chunk_size - 1 {
        chunks.push("[DONE]".to_string());
      }
    }

    Ok(chunks)
  }

  fn serialize_model_client(&self, client: &ModelClient) -> JsonValue {
    match client {
      ModelClient::OpenAI(model) => json!({
          "type": "openai",
          "model": model.model,
          "base_url": model.base_url,
      }),
      ModelClient::Anthropic(model) => json!({
          "type": "anthropic",
          "model": model.model,
          "base_url": model.base_url,
      }),
      ModelClient::Local(model) => json!({
          "type": "local",
          "endpoint": model.endpoint,
          "model_name": model.model_name,
      }),
      ModelClient::Custom(model) => json!({
          "type": "custom",
          "provider": model.provider,
          "config": model.config,
      }),
    }
  }

  fn get_model_capabilities(&self, config: &LlmConfig) -> ModelCapabilities {
    match config.provider.as_str() {
      "openai" => ModelCapabilities {
        chat: true,
        completion: true,
        tools: true,
        streaming: true,
        function_calling: true,
        vision: config.model.contains("vision") || config.model.contains("4o"),
        max_context_length: Some(self.get_context_length(&config.model)),
      },
      "anthropic" => ModelCapabilities {
        chat: true,
        completion: false,
        tools: true,
        streaming: true,
        function_calling: true,
        vision: config.model.contains("vision"),
        max_context_length: Some(self.get_context_length(&config.model)),
      },
      "local" => ModelCapabilities {
        chat: true,
        completion: false,
        tools: false,     // å–å†³äºå…·ä½“å®ç°
        streaming: false, // å–å†³äºå…·ä½“å®ç°
        function_calling: false,
        vision: false,
        max_context_length: None, // å–å†³äºå…·ä½“æ¨¡å‹
      },
      _ => ModelCapabilities::default(),
    }
  }

  fn get_context_length(&self, model: &str) -> usize {
    // ç®€åŒ–çš„ä¸Šä¸‹æ–‡é•¿åº¦æ˜ å°„
    if model.contains("gpt-4") {
      8192
    } else if model.contains("gpt-3.5") {
      4096
    } else if model.contains("claude-3") {
      200000
    } else {
      4096 // é»˜è®¤å€¼
    }
  }
}

// Helper function for creating ExecutionDataMap
fn make_execution_data_map(data: Vec<(&str, ExecutionDataItems)>) -> ExecutionDataMap {
  use fusion_common::ahash::{HashMap, HashMapExt};
  let mut map = HashMap::new();
  for (key, value) in data {
    let connection_kind = match key {
      "main" => ConnectionKind::Main,
      "error" => ConnectionKind::Error,
      "ai_agent" => ConnectionKind::AiAgent,
      "ai_tool" => ConnectionKind::AiTool,
      "ai_model" => ConnectionKind::AiModel,
      "ai_output_parser" => ConnectionKind::AiOutputParser,
      "ai_memory" => ConnectionKind::AiMemory,
      "ai_document" => ConnectionKind::AiDocument,
      "ai_embedding" => ConnectionKind::AiEmbedding,
      "ai_retriever" => ConnectionKind::AiRetriever,
      "ai_vector_store" => ConnectionKind::AiVectorStore,
      "ai_reranker" => ConnectionKind::AiReranker,
      "ai_text_splitter" => ConnectionKind::AiTextSplitter,
      _ => continue, // Skip unknown connection types
    };
    map.insert(connection_kind, vec![value]);
  }
  map
}
