# Cluster Node æ¶æ„è®¾è®¡æ–‡æ¡£

## æ¦‚è¿°

æœ¬æ–‡æ¡£æè¿°äº†åŸºäº n8n Cluster Node æ¶æ„çš„é›†æˆæ–¹æ¡ˆï¼Œå°†ç°æœ‰çš„ AiAgentV1ã€DeepSeek LLMã€Memory èŠ‚ç‚¹é‡æ„ä¸º Root Node + Sub Nodes çš„æ¶æ„æ¨¡å¼ï¼Œé€šè¿‡ fusion-ai çš„ graph_flow æ¡†æ¶å®ç°ç»Ÿä¸€çš„æ‰§è¡Œç®¡ç†ã€‚

## ğŸ¯ ç»Ÿä¸€æŠ€æœ¯è§„èŒƒ

### ç±»å‹ç³»ç»Ÿ

```rust
/// Sub Node Provider ç±»å‹åˆ«åï¼Œç»Ÿä¸€ä½¿ç”¨ Arc åŒ…è£…
pub type SubNodeProviderRef = Arc<dyn SubNodeProvider>;
pub type LLMSubNodeProviderRef = Arc<dyn LLMSubNodeProvider>;
pub type MemorySubNodeProviderRef = Arc<dyn MemorySubNodeProvider>;
pub type ToolSubNodeProviderRef = Arc<dyn ToolSubNodeProvider>;
```

### æ¥å£è®¾è®¡åŸåˆ™

1. **ç»Ÿä¸€ä½¿ç”¨ Arc<dyn Trait>**ï¼šé¿å…æ‰€æœ‰æƒé—®é¢˜ï¼Œæ”¯æŒå…±äº«å¼•ç”¨
2. **ç›´æ¥æ‰©å±• NodeRegistry**ï¼šä¸ä½¿ç”¨åŒ…è£…å™¨ï¼Œä¿æŒ API ä¸€è‡´æ€§
3. **ç±»å‹å®‰å…¨çš„ downcast**ï¼šé€šè¿‡ provider_type() + downcast_ref() å®ç°å®‰å…¨è½¬æ¢
4. **é”™è¯¯å¤„ç†ç»Ÿä¸€**ï¼šæ‰©å±•ç°æœ‰ NodeExecutionErrorï¼Œä¸åˆ›å»ºæ–°çš„é”™è¯¯å±‚æ¬¡

### æ¶æ„å†³ç­–

- âœ… **Trait SubNodeProvider æ–¹æ¡ˆ**ï¼ˆå·²ç¡®å®šï¼‰
- âœ… **Arc<dyn SubNodeProvider> ç±»å‹**ï¼ˆå·²ç¡®å®šï¼‰
- âœ… **ç›´æ¥æ‰©å±• NodeRegistry**ï¼ˆå·²ç¡®å®šï¼‰
- âœ… **æ¸è¿›å¼é‡æ„ç­–ç•¥**ï¼ˆå·²ç¡®å®šï¼‰

### é‡æ„ç›®æ ‡

1. **ä¿æŒå‘åå…¼å®¹**ï¼šç°æœ‰ AiAgentV1 ç»§ç»­å¯ç”¨ï¼Œç¡®ä¿ç°æœ‰å·¥ä½œæµä¸å—å½±å“
2. **æ¨¡å—åŒ–æ¶æ„**ï¼šé€šè¿‡ SubNodeProvider trait å®ç°æ¾è€¦åˆçš„ç»„ä»¶åŒ–è®¾è®¡
3. **ç»Ÿä¸€æ‰§è¡Œç®¡ç†**ï¼šä½¿ç”¨ GraphFlow ç®¡ç†æ•´ä¸ª Cluster Node çš„ç”Ÿå‘½å‘¨æœŸ
4. **å¯æ‰©å±•æ€§**ï¼šæ”¯æŒåŠ¨æ€æ·»åŠ æ–°çš„ Sub Node ç±»å‹ï¼ˆLLMã€Memoryã€Toolï¼‰

## 1. æ¶æ„è®¾è®¡

### 1.1 æ ¸å¿ƒæ¦‚å¿µ

- **Root Node**ï¼šAiAgentV1 ä½œä¸ºä¸»åè°ƒèŠ‚ç‚¹ï¼Œè´Ÿè´£ç®¡ç†æ•´ä¸ª Cluster Node çš„ç”Ÿå‘½å‘¨æœŸ
- **Sub Nodes**ï¼šç‹¬ç«‹çš„ LLMã€Memoryã€Tool èŠ‚ç‚¹ï¼Œæä¾›ä¸“ä¸šåŒ–åŠŸèƒ½
- **è¿æ¥ç®¡ç†**ï¼šé€šè¿‡ ConnectionKind ç®¡ç†ä¸åŒç±»å‹ Sub Nodes çš„è¿æ¥å…³ç³»
- **æ‰§è¡Œåè°ƒ**ï¼šä½¿ç”¨ fusion_ai::graph_flow::FlowRunner ç»Ÿä¸€ç®¡ç† Sub Nodes çš„æ‰§è¡Œ

### 1.2 æ¶æ„æ¨¡å¼

```rust
/// Sub Node Provider trait å±‚æ¬¡ç»“æ„
use async_trait::async_trait;
use fusion_ai::graph_flow::{Context, NextAction, Task, TaskResult};
use hetumind_core::workflow::{NodeDefinition, NodeExecutionError, NodeKind};
use rig::completion::Chat;
use rig::message::Message;
use rig::prelude::*;
use serde_json::json;
use std::sync::Arc;

/// Sub Node Provider ç±»å‹åˆ«åï¼Œç»Ÿä¸€ä½¿ç”¨ Arc åŒ…è£…
pub type SubNodeProviderRef = Arc<dyn SubNodeProvider>;
pub type LLMSubNodeProviderRef = Arc<dyn LLMSubNodeProvider>;
pub type MemorySubNodeProviderRef = Arc<dyn MemorySubNodeProvider>;
pub type ToolSubNodeProviderRef = Arc<dyn ToolSubNodeProvider>;

/// Sub Node Provider åŸºç¡€ trait
#[async_trait]
pub trait SubNodeProvider: Send + Sync {
    fn provider_type(&self) -> SubNodeProviderType;
    fn get_node_definition(&self) -> Arc<NodeDefinition>;
    async fn initialize(&self) -> Result<(), NodeExecutionError>;
}

/// Sub Node Provider ç±»å‹
#[derive(Debug, Clone, PartialEq, Eq)]
pub enum SubNodeProviderType {
    LLM,
    Memory,
    Tool,
}
```

## 2. Sub Node Provider æ¥å£è®¾è®¡

### 2.1 LLM Sub Node Provider

```rust
/// LLM Sub Node Provider æ¥å£
#[async_trait]
pub trait LLMSubNodeProvider: SubNodeProvider {
    async fn call_llm(&self, messages: Vec<Message>, config: LLMConfig) -> Result<LLMResponse, NodeExecutionError>;
}

/// LLM é…ç½®
#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]
pub struct LLMConfig {
    pub model: String,
    pub max_tokens: Option<u32>,
    pub temperature: Option<f64>,
    pub top_p: Option<u32>,
    pub stop_sequences: Option<Vec<String>>,
    pub api_key: Option<String>,
}

/// LLM å“åº”
#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]
pub struct LLMResponse {
    pub content: String,
    pub role: String,
    pub usage: Option<UsageStats>,
}

/// ä½¿ç”¨ç»Ÿè®¡ - å¤ç”¨ç°æœ‰çš„ UsageStats
pub use crate::llm::shared::UsageStats;
```

### 2.2 Memory Sub Node Provider

```rust
/// Memory Sub Node Provider æ¥å£
#[async_trait]
pub trait MemorySubNodeProvider: SubNodeProvider {
    async fn store_messages(&self, session_id: &str, messages: Vec<Message>) -> Result<(), NodeExecutionError>;
    async fn retrieve_messages(&self, session_id: &str, count: usize) -> Result<Vec<Message>, NodeExecutionError>;
}
```

### 2.3 Tool Sub Node Provider

```rust
/// Tool Sub Node Provider æ¥å£
#[async_trait]
pub trait ToolSubNodeProvider: SubNodeProvider {
    async fn as_rig_tool(&self) -> Result<rig::tool::Tool, NodeExecutionError>;
}
```

## 3. AiAgentV1 é‡æ„æ–¹æ¡ˆ

### 3.1 æ–°å¢ Sub Node æ”¶é›†åŠŸèƒ½

```rust
use fusion_common::time::now_offset;
use fusion_ai::graph_flow::{
    Context, FlowRunner, GraphBuilder, GraphError, GraphStorage, InMemoryGraphStorage,
    InMemorySessionStorage, NextAction, Session, SessionStorage, Task, TaskResult,
};
use hetumind_core::types::JsonValue;
use hetumind_core::version::Version;
use hetumind_core::workflow::{
    ConnectionKind, DataSource, ExecutionData, ExecutionDataItems, ExecutionDataMap, NodeDefinition, NodeExecutable,
    NodeExecutionContext, NodeExecutionError, NodeSupplier, RegistrationError, SupplyResult,
};
use log::{debug, info, warn};
use mea::rwlock::RwLock;
use rig::message::{Message, UserContent};
use serde_json::json;
use std::sync::Arc;

use crate::cluster::ai_agent::tool_manager::ToolManager;
use crate::cluster::ai_agent::utils::create_base_definition;
use crate::memory::simple_memory_node::{ConversationMessage, MessageRole, SimpleMemoryAccessor};
use crate::memory::graph_flow_memory::GraphFlowMemoryManager;

use super::parameters::AiAgentConfig;

/// é‡æ„åçš„ AiAgentV1
pub struct AiAgentV1Refactored {
    pub definition: Arc<NodeDefinition>,
    tool_manager: Arc<RwLock<ToolManager>>,
    memory_manager: Arc<GraphFlowMemoryManager>,
}

impl AiAgentV1Refactored {
    pub fn new() -> Result<Self, RegistrationError> {
        let base = create_base_definition();
        let definition = base.with_version(Version::new(2, 0, 0));

        Ok(Self {
            definition: Arc::new(definition),
            tool_manager: Arc::new(RwLock::new(ToolManager::new())),
            memory_manager: Arc::new(GraphFlowMemoryManager::new()),
        })
    }
}

#[async_trait]
impl NodeExecutable for AiAgentV1Refactored {
    async fn execute(&self, context: &NodeExecutionContext) -> Result<ExecutionDataMap, NodeExecutionError> {
        info!("Executing refactored AiAgent V2 with Cluster Node architecture");

        // 1. è·å–é…ç½®
        let config: AiAgentConfig = context.get_parameters()?;

        // 2. æ”¶é›†æ‰€æœ‰ Sub Nodes
        let sub_nodes = self.collect_sub_nodes(context).await?;

        // 3. å‡†å¤‡è¾“å…¥æ¶ˆæ¯
        let input_data = context.get_input_data(ConnectionKind::Main)?;
        let current_input = input_data.get_value::<Message>("prompt")?;

        // 4. ä½¿ç”¨ GraphFlow æ‰§è¡Œ Cluster
        let result = self.execute_cluster_with_graphflow(sub_nodes, current_input, &config).await?;

        // 5. æ„å»ºè¿”å›æ•°æ®
        let mut data_map = ExecutionDataMap::default();
        data_map.insert(
            ConnectionKind::Main,
            vec![ExecutionDataItems::new_item(ExecutionData::new_json(
                json!({
                    "response": result,
                    "node_kind": &self.definition().kind,
                    "version": "2.0.0",
                    "architecture": "cluster_node",
                    "timestamp": now_offset(),
                }),
                Some(DataSource::new(context.current_node_name().clone(), ConnectionKind::Main, 0)),
            ))],
        );

        Ok(data_map)
    }

    fn definition(&self) -> Arc<NodeDefinition> {
        Arc::clone(&self.definition)
    }
}

impl AiAgentV1Refactored {
    /// ä» Workflow ä¸Šä¸‹æ–‡ä¸­æ”¶é›†æ‰€æœ‰ Sub Nodes
    async fn collect_sub_nodes(&self, context: &NodeExecutionContext) -> Result<Vec<SubNodeProviderRef>, NodeExecutionError> {
        let mut sub_nodes = Vec::new();

        // 1. æ”¶é›† LLM Sub Node (å¿…éœ€)
        let llm_conn = context
            .workflow
            .connections
            .get(context.current_node_name())
            .and_then(|kind_conns| kind_conns.get(&ConnectionKind::AiLM))
            .and_then(|conns| conns.iter().next())
            .ok_or_else(|| NodeExecutionError::ConfigurationError("No LLM Sub Node connection found".to_string()))?;

        let node = context.workflow.get_node(llm_conn.node_name())
            .ok_or_else(|| NodeExecutionError::ConnectionError(format!("LLM node not found: {}", llm_conn.node_name())))?;

        let provider = context.node_registry.get_subnode_provider(&node.kind)
            .ok_or_else(|| NodeExecutionError::ConfigurationError(format!("No SubNodeProvider found for: {}", node.kind)))?;

        let llm_provider = provider
            .downcast_ref::<Arc<dyn LLMSubNodeProvider>>()
            .cloned()
            .ok_or_else(|| NodeExecutionError::ConfigurationError("Provider is not an LLM provider".to_string()))?;

        sub_nodes.push(llm_provider);

        // 2. æ”¶é›† Memory Sub Node (å¯é€‰)
        if let Some(memory_conn) = context
            .workflow
            .connections
            .get(context.current_node_name())
            .and_then(|kind_conns| kind_conns.get(&ConnectionKind::AiMemory))
            .and_then(|conns| conns.iter().next())
        {
            let node = context.workflow.get_node(memory_conn.node_name())
                .ok_or_else(|| NodeExecutionError::ConnectionError(format!("Memory node not found: {}", memory_conn.node_name())))?;

            let provider = context.node_registry.get_subnode_provider(&node.kind)
                .ok_or_else(|| NodeExecutionError::ConfigurationError(format!("No SubNodeProvider found for: {}", node.kind)))?;

            let memory_provider = provider
                .downcast_ref::<Arc<dyn MemorySubNodeProvider>>()
                .cloned()
                .ok_or_else(|| NodeExecutionError::ConfigurationError("Provider is not a Memory provider".to_string()))?;

            sub_nodes.push(memory_provider);
        }

        // 3. æ”¶é›† Tool Sub Nodes (å¯é€‰ï¼Œå¤šä¸ª)
        if let Some(tool_conns) = context
            .workflow
            .connections
            .get(context.current_node_name())
            .and_then(|kind_conns| kind_conns.get(&ConnectionKind::AiTool))
            .map(|conns| conns.iter().collect::<Vec<_>>())
        {
            let mut tool_providers = Vec::new();

            for conn in tool_conns {
                let node = context.workflow.get_node(conn.node_name())
                    .ok_or_else(|| NodeExecutionError::ConnectionError(format!("Tool node not found: {}", conn.node_name())))?;

                let provider = context.node_registry.get_subnode_provider(&node.kind)
                    .ok_or_else(|| NodeExecutionError::ConfigurationError(format!("No SubNodeProvider found for: {}", node.kind)))?;

                let tool_provider = provider
                    .downcast_ref::<Arc<dyn ToolSubNodeProvider>>()
                    .cloned()
                    .ok_or_else(|| NodeExecutionError::ConfigurationError("Provider is not a Tool provider".to_string()))?;

                tool_providers.push(tool_provider);
            }

            sub_nodes.extend(tool_providers);
        }

        info!("Collected {} sub_nodes for cluster execution", sub_nodes.len());
        Ok(sub_nodes)
    }

    /// ä½¿ç”¨ GraphFlow æ‰§è¡Œ Cluster
    async fn execute_cluster_with_graphflow(
        &self,
        sub_nodes: Vec<SubNodeProviderRef>,
        initial_input: Message,
        config: &AiAgentConfig,
    ) -> Result<JsonValue, NodeExecutionError> {
        // 1. æ„å»º Graph
        let (graph, session_id) = self.build_cluster_graph(sub_nodes, initial_input, config).await?;

        // 2. åˆ›å»º Session Storage
        let session_storage: Arc<dyn SessionStorage> = Arc::new(InMemorySessionStorage::new());

        // 3. åˆ›å»º FlowRunner
        let runner = FlowRunner::new(graph, session_storage);

        // 4. å¾ªç¯æ‰§è¡Œç›´åˆ°å®Œæˆ
        let result = self.execute_flow_runner(runner, session_id).await?;

        Ok(result)
    }

    /// æ„å»º Cluster Graph
    async fn build_cluster_graph(
        &self,
        sub_nodes: Vec<SubNodeProviderRef>,
        initial_input: Message,
        config: &AiAgentConfig,
    ) -> Result<(Arc<Graph>, String), NodeExecutionError> {
        let mut graph_builder = GraphBuilder::new("ai_agent_cluster_v2");
        let session_id = format!("cluster_session_{}", uuid::Uuid::new_v4());

        // 1. åˆ›å»ºæ¶ˆæ¯å‡†å¤‡ Task
        let prep_task = Arc::new(MessagePreparationTask::new(initial_input, config.system_prompt().unwrap_or("")));
        graph_builder = graph_builder.add_task(prep_task.clone());

        // 2. å¤„ç† Sub Nodes
        let mut llm_task = None;
        let mut memory_task = None;

        for sub_node in sub_nodes {
            match sub_node.provider_type() {
                SubNodeProviderType::LLM => {
                    let llm_provider = sub_node
                        .downcast_ref::<Arc<dyn LLMSubNodeProvider>>()
                        .cloned()
                        .ok_or_else(|| NodeExecutionError::ConfigurationError("Failed to downcast LLM provider".to_string()))?;
                    let task = Arc::new(LLMProviderTask::new(llm_provider));
                    graph_builder = graph_builder.add_task(task.clone());
                    llm_task = Some(task);
                }
                SubNodeProviderType::Memory => {
                    let memory_provider = sub_node
                        .downcast_ref::<Arc<dyn MemorySubNodeProvider>>()
                        .cloned()
                        .ok_or_else(|| NodeExecutionError::ConfigurationError("Failed to downcast Memory provider".to_string()))?;
                    let task = Arc::new(MemoryProviderTask::new(memory_provider));
                    graph_builder = graph_builder.add_task(task.clone());
                    memory_task = Some(task);
                }
                SubNodeProviderType::Tool => {
                    // å¤„ç†å·¥å…· Providersï¼ˆåç»­å®ç°ï¼‰
                    debug!("Found tool provider");
                }
            }
        }

        // 3. åˆ›å»ºå“åº”åå¤„ç† Task
        let post_task = Arc::new(ResponsePostProcessTask::new());
        graph_builder = graph_builder.add_task(post_task.clone());

        // 4. å»ºç«‹ Task è¿æ¥å…³ç³»
        if let Some(llm_task) = &llm_task {
            graph_builder = graph_builder
                .add_edge(prep_task.id(), llm_task.id());

            if let Some(memory_task) = &memory_task {
                graph_builder = graph_builder
                    .add_edge(llm_task.id(), memory_task.id())
                    .add_edge(memory_task.id(), post_task.id());
            } else {
                graph_builder = graph_builder.add_edge(llm_task.id(), post_task.id());
            }
        }

        let graph = Arc::new(graph_builder.build());

        // 5. åˆ›å»º Session
        let session = Session::new_from_task(session_id.clone(), prep_task.id());
        session.context.set("session_id", session_id.clone()).await;
        session.context.set("config", config.clone()).await;

        Ok((graph, session_id))
    }

    /// æ‰§è¡Œ FlowRunner
    async fn execute_flow_runner(&self, runner: FlowRunner, session_id: String) -> Result<JsonValue, NodeExecutionError> {
        loop {
            let execution_result = runner.run(&session_id)
                .await
                .map_err(|e| NodeExecutionError::ExternalServiceError {
                    service: format!("GraphFlow execution error: {}", e)
                })?;

            match execution_result.status {
                ExecutionStatus::Completed => {
                    if let Some(response) = execution_result.response {
                        let result_json: JsonValue = serde_json::from_str(&response)
                            .map_err(|e| NodeExecutionError::InvalidInput(format!("Failed to parse response: {}", e)))?;
                        return Ok(result_json);
                    } else {
                        return Err(NodeExecutionError::ConfigurationError("No response from completed execution".to_string()));
                    }
                }
                ExecutionStatus::Paused { next_task_id, reason } => {
                    info!("Cluster execution paused, continuing to task: {} (reason: {})", next_task_id, reason);
                    continue;
                }
                ExecutionStatus::WaitingForInput => {
                    info!("Cluster execution waiting for input, continuing...");
                    continue;
                }
                ExecutionStatus::Error(e) => {
                    return Err(NodeExecutionError::ExternalServiceError {
                        service: format!("GraphFlow execution failed: {}", e)
                    });
                }
            }
        }
    }
}
```

## 4. DeepSeek LLM Sub Node Provider å®ç°

### 4.1 Provider å®ç°

```rust
/// DeepSeek LLM Sub Node Provider å®ç°
pub struct DeepSeekLLMSubNodeProvider {
    config: DeepSeekNodeConfig,
    definition: Arc<NodeDefinition>,
}

impl DeepSeekLLMSubNodeProvider {
    pub fn new(config: DeepSeekNodeConfig, definition: Arc<NodeDefinition>) -> Self {
        Self { config, definition }
    }

    /// ä»ç°æœ‰ DeepSeek V1 é…ç½®åˆ›å»º Provider
    pub fn from_existing_deepseek_v1(deepseek_v1: &crate::llm::deepseek_node::deepseek_v1::DeepseekV1, config: DeepSeekNodeConfig) -> Self {
        Self {
            config,
            definition: deepseek_v1.definition(),
        }
    }
}

#[async_trait]
impl SubNodeProvider for DeepSeekLLMSubNodeProvider {
    fn provider_type(&self) -> SubNodeProviderType {
        SubNodeProviderType::LLM
    }

    fn get_node_definition(&self) -> Arc<NodeDefinition> {
        Arc::clone(&self.definition)
    }

    async fn initialize(&self) -> Result<(), NodeExecutionError> {
        // éªŒè¯ API å¯†é’¥
        let resolved_api_key = crate::llm::shared::resolve_api_key(&self.config.common.api_key).await?;
        crate::llm::shared::validate_api_key_resolved(&resolved_api_key, "DeepSeek")?;
        Ok(())
    }
}

#[async_trait]
impl LLMSubNodeProvider for DeepSeekLLMSubNodeProvider {
    async fn call_llm(&self, messages: Vec<Message>, _config: LLMConfig) -> Result<LLMResponse, NodeExecutionError> {
        // è§£æ API å¯†é’¥
        let resolved_api_key = crate::llm::shared::resolve_api_key(&self.config.common.api_key).await?;
        let api_key = crate::llm::shared::validate_api_key_resolved(&resolved_api_key, "DeepseekLLMProvider")?;

        // åˆ›å»º DeepSeek å®¢æˆ·ç«¯
        let deepseek_client = rig::providers::deepseek::Client::new(&api_key);

        // æ„å»º Agent
        let mut ab = deepseek_client.agent(&self.config.model);
        ab = crate::llm::set_agent_builder(&messages, ab);
        let agent = ab.build();

        // æå–æœ€åä¸€ä¸ªæ¶ˆæ¯ä½œä¸º prompt
        let prompt = messages.last()
            .cloned()
            .ok_or_else(|| NodeExecutionError::InvalidInput("No messages provided".to_string()))?;

        let completion = agent.completion(prompt, vec![]).await
            .map_err(|e| NodeExecutionError::ExternalServiceError {
                service: format!("DeepSeek agent completion error: {}", e),
            })?;

        let completion_response = completion.send().await
            .map_err(|e| crate::llm::complation_error_2_execution_error("deepseek_llm_provider".to_string(), e))?;

        // è§£æå“åº”
        let usage_stats = UsageStats::from(completion_response.usage);
        let response_text = extract_response_text(&completion_response);

        Ok(LLMResponse {
            content: response_text,
            role: "assistant".to_string(),
            usage: Some(usage_stats),
        })
    }
}

/// ä» rig-core å“åº”ä¸­æå–æ–‡æœ¬
fn extract_response_text(completion_response: &rig::completion::CompletionResponse) -> String {
    if let Some(choice) = completion_response.choice.first() {
        match choice {
            rig::message::AssistantContent::Text(text) => text.text.clone(),
            _ => "".to_string(),
        }
    } else {
        "".to_string()
    }
}
```

### 4.2 GraphFlow é›†æˆä»»åŠ¡

```rust
/// LLM Provider GraphFlow Task
pub struct LLMProviderTask {
    llm_provider: Arc<dyn LLMSubNodeProvider>,
}

impl LLMProviderTask {
    pub fn new(llm_provider: Arc<dyn LLMSubNodeProvider>) -> Self {
        Self { llm_provider }
    }
}

#[async_trait]
impl Task for LLMProviderTask {
    async fn run(&self, context: Context) -> fusion_ai::graph_flow::Result<TaskResult> {
        // è·å–è¾“å…¥æ¶ˆæ¯
        let messages: Vec<Message> = context.get_sync("input_messages").unwrap_or_default();
        let config: LLMConfig = context.get_sync("llm_config").unwrap_or_default();

        // è°ƒç”¨ LLM
        let response = self.llm_provider.call_llm(messages, config)
            .await
            .map_err(|e| GraphError::TaskExecutionFailed(e.to_string()))?;

        // è®¾ç½®ç»“æœåˆ°ä¸Šä¸‹æ–‡
        context.set("llm_response", response).await;

        Ok(TaskResult::new(None, NextAction::Continue))
    }

    fn id(&self) -> &str {
        "llm_provider_task"
    }
}
```

## 5. Memory Sub Node Provider å®ç°

### 5.1 Provider å®ç°

```rust
/// Simple Memory Sub Node Provider å®ç°
pub struct SimpleMemorySubNodeProvider {
    memory_manager: Arc<GraphFlowMemoryManager>,
    session_id: String,
    definition: Arc<NodeDefinition>,
}

impl SimpleMemorySubNodeProvider {
    pub fn new(
        memory_manager: Arc<GraphFlowMemoryManager>,
        session_id: String,
        definition: Arc<NodeDefinition>,
    ) -> Self {
        Self {
            memory_manager,
            session_id,
            definition,
        }
    }

    /// ä»ç°æœ‰ simple_memory_node åˆ›å»º Provider
    pub fn from_existing_simple_memory(
        simple_memory: &crate::memory::simple_memory_node::simple_memory_v1::SimpleMemoryV1,
        memory_manager: Arc<GraphFlowMemoryManager>,
        session_id: String,
    ) -> Self {
        Self {
            memory_manager,
            session_id,
            definition: simple_memory.definition(),
        }
    }
}

#[async_trait]
impl SubNodeProvider for SimpleMemorySubNodeProvider {
    fn provider_type(&self) -> SubNodeProviderType {
        SubNodeProviderType::Memory
    }

    fn get_node_definition(&self) -> Arc<NodeDefinition> {
        Arc::clone(&self.definition)
    }

    async fn initialize(&self) -> Result<(), NodeExecutionError> {
        // Memory provider æ— éœ€ç‰¹æ®Šåˆå§‹åŒ–
        Ok(())
    }
}

#[async_trait]
impl MemorySubNodeProvider for SimpleMemorySubNodeProvider {
    async fn store_messages(&self, session_id: &str, messages: Vec<Message>) -> Result<(), NodeExecutionError> {
        let message_values: Vec<JsonValue> = messages
            .into_iter()
            .map(|msg| {
                json!({
                    "role": self.message_role_to_string(msg.role),
                    "content": self.message_content_to_string(msg.content),
                    "timestamp": now_offset(),
                })
            })
            .collect();

        self.memory_manager.store_messages(session_id, "ai_agent_workflow", message_values, None)
            .await
            .map_err(|e| NodeExecutionError::ExternalServiceError {
                service: format!("Memory storage error: {}", e)
            })?;
        Ok(())
    }

    async fn retrieve_messages(&self, session_id: &str, count: usize) -> Result<Vec<Message>, NodeExecutionError> {
        let messages = self.memory_manager.retrieve_messages(session_id, count)
            .await
            .map_err(|e| NodeExecutionError::ExternalServiceError {
                service: format!("Memory retrieval error: {}", e)
            })?;

        let rig_messages: Vec<Message> = messages
            .into_iter()
            .map(|msg| {
                let content = match msg.content {
                    crate::memory::graph_flow_memory::UserContent::Text(text) => UserContent::Text(text.text),
                    crate::memory::graph_flow_memory::UserContent::Image(_) => UserContent::Text("image_content".to_string()),
                };

                match msg.role.as_str() {
                    "user" => Message::user(content),
                    "assistant" => Message::assistant(content),
                    "system" => Message::system(content),
                    _ => Message::user(content),
                }
            })
            .collect();

        Ok(rig_messages)
    }
}

impl SimpleMemorySubNodeProvider {
    fn message_role_to_string(&self, role: MessageRole) -> String {
        match role {
            MessageRole::System => "system".to_string(),
            MessageRole::User => "user".to_string(),
            MessageRole::Assistant => "assistant".to_string(),
            MessageRole::Tool => "tool".to_string(),
        }
    }

    fn message_content_to_string(&self, content: UserContent) -> String {
        match content {
            UserContent::Text(text) => text.text,
            UserContent::Image(_) => "image_content".to_string(),
        }
    }
}
```

### 5.2 GraphFlow é›†æˆä»»åŠ¡

```rust
/// Memory Provider GraphFlow Task
pub struct MemoryProviderTask {
    memory_provider: Arc<dyn MemorySubNodeProvider>,
}

impl MemoryProviderTask {
    pub fn new(memory_provider: Arc<dyn MemorySubNodeProvider>) -> Self {
        Self { memory_provider }
    }
}

#[async_trait]
impl Task for MemoryProviderTask {
    async fn run(&self, context: Context) -> fusion_ai::graph_flow::Result<TaskResult> {
        let action: String = context.get_sync("memory_action").unwrap_or_else(|| "retrieve".to_string());
        let session_id: String = context.get_sync("session_id").unwrap_or_else(|| "default_session".to_string());

        match action.as_str() {
            "store" => {
                let messages: Vec<Message> = context.get_sync("messages").unwrap_or_default();
                self.memory_provider.store_messages(&session_id, messages)
                    .await
                    .map_err(|e| GraphError::TaskExecutionFailed(e.to_string()))?;
            }
            "retrieve" => {
                let count: usize = context.get_sync("count").unwrap_or(5);
                let messages = self.memory_provider.retrieve_messages(&session_id, count)
                    .await
                    .map_err(|e| GraphError::TaskExecutionFailed(e.to_string()))?;
                context.set("retrieved_messages", messages).await;
            }
            _ => {
                return Err(GraphError::TaskExecutionFailed(
                    format!("Unknown memory action: {}", action)
                ));
            }
        }

        Ok(TaskResult::new(None, NextAction::Continue))
    }

    fn id(&self) -> &str {
        "memory_provider_task"
    }
}
```

## 6. NodeRegistry æ‰©å±•

### 6.1 ç›´æ¥æ‰©å±• NodeRegistry æ”¯æŒ Provider

```rust
/// æ‰©å±•ç°æœ‰ NodeRegistry ä»¥æ”¯æŒ Sub Node Provider
impl NodeRegistry {
    /// æ³¨å†Œ Sub Node Provider
    pub fn register_subnode_provider(
        &mut self,
        kind: NodeKind,
        provider: SubNodeProviderRef,
    ) -> Result<(), RegistrationError> {
        // ä½¿ç”¨å†…éƒ¨ HashMap å­˜å‚¨ providers
        if let Some(providers) = self.subnode_providers_mut() {
            providers.insert(kind, provider);
            Ok(())
        } else {
            Err(RegistrationError::InternalError("Failed to access subnode providers".to_string()))
        }
    }

    /// è·å– Sub Node Provider
    pub fn get_subnode_provider(&self, kind: &NodeKind) -> Option<SubNodeProviderRef> {
        self.subnode_providers()
            .and_then(|providers| providers.get(kind).cloned())
    }

    /// æ ¹æ®ç±»å‹è·å– Provider
    pub fn get_providers_by_type(&self, provider_type: SubNodeProviderType) -> Vec<SubNodeProviderRef> {
        self.subnode_providers()
            .map(|providers| {
                providers
                    .values()
                    .filter(|provider| provider.provider_type() == provider_type)
                    .cloned()
                    .collect()
            })
            .unwrap_or_default()
    }
}
```

## 7. Workflow é›†æˆç¤ºä¾‹

### 7.1 Workflow å®šä¹‰

```rust
/// åœ¨ Workflow ä¸­é…ç½® Cluster Node
let mut workflow = Workflow::new(workflow_id, "AI_Agent_Cluster");

// 1. Root Node: AiAgentV1
let ai_agent_node = NodeElement::new("ai_agent_root", "ai_agent_v1_refactored");
workflow.add_node(ai_agent_node);

// 2. LLM Sub Node: DeepSeek
let deepseek_node = NodeElement::new("deepseek_llm", "deepseek_llm_provider");
workflow.add_node(deepseek_node);

// 3. Memory Sub Node: Simple Memory (å¯é€‰)
let memory_node = NodeElement::new("simple_memory", "simple_memory_provider");
workflow.add_node(memory_node);

// 4. Tool Sub Nodes: Calculator & Search (å¯é€‰ï¼Œå¤šä¸ª)
let calculator_node = NodeElement::new("calculator_tool", "calculator_tool_provider");
let search_node = NodeElement::new("search_tool", "search_tool_provider");
workflow.add_node(calculator_node);
workflow.add_node(search_node);

// 5. å®šä¹‰è¿æ¥å…³ç³»
workflow.add_connection(
    "ai_agent_root",
    &ConnectionKind::AiLM,
    vec![Connection::new("deepseek_llm", ConnectionKind::AiLM)]
);

workflow.add_connection(
    "ai_agent_root",
    &ConnectionKind::AiMemory,
    vec![Connection::new("simple_memory", ConnectionKind::AiMemory)]
);

workflow.add_connection(
    "ai_agent_root",
    &ConnectionKind::AiTool,
    vec![
        Connection::new("calculator_tool", ConnectionKind::AiTool),
        Connection::new("search_tool", ConnectionKind::AiTool)
    ]
);
```

## 8. é…ç½®ç®¡ç†

### 8.1 Sub Node é…ç½®

```rust
/// Cluster Node å­èŠ‚ç‚¹é…ç½®
#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]
pub struct ClusterNodeConfig {
    /// LLM é…ç½®
    pub llm_config: Option<LLMConfig>,
    /// Memory é…ç½®
    pub memory_config: Option<MemoryConfig>,
    /// Tool é…ç½®
    pub tools_config: Option<Vec<ToolConfig>>,
    /// æ‰§è¡Œé…ç½®
    pub execution_config: ExecutionConfig,
}

/// æ‰§è¡Œé…ç½®
#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]
pub struct ExecutionConfig {
    pub timeout_seconds: Option<u64>,
    pub max_retries: Option<u32>,
    pub parallel_execution: Option<bool>,
}
```

### 8.2 é…ç½®éªŒè¯

```rust
impl ClusterNodeConfig {
    /// éªŒè¯é…ç½®çš„æœ‰æ•ˆæ€§
    pub fn validate(&self) -> Result<(), NodeExecutionError> {
        // éªŒè¯ LLM é…ç½®
        if let Some(llm_config) = &self.llm_config {
            if llm_config.model.is_empty() {
                return Err(NodeExecutionError::ConfigurationError("LLM model cannot be empty".to_string()));
            }
        }

        // éªŒè¯ Memory é…ç½®
        if let Some(memory_config) = &self.memory_config {
            if let Some(context_window) = memory_config.context_window {
                if context_window == 0 {
                    return Err(NodeExecutionError::ConfigurationError("Context window must be greater than 0".to_string()));
                }
            }
        }

        Ok(())
    }
}
```

## 9. é”™è¯¯å¤„ç†

### 9.1 ç»Ÿä¸€é”™è¯¯å¤„ç†ç­–ç•¥

åŸºäºæ¶æ„å†³ç­–ï¼Œæˆ‘ä»¬é‡‡ç”¨ç»Ÿä¸€çš„é”™è¯¯å¤„ç†ç­–ç•¥ï¼Œä¸åˆ›å»ºæ–°çš„é”™è¯¯å±‚æ¬¡ï¼Œè€Œæ˜¯æ‰©å±•ç°æœ‰çš„ `NodeExecutionError`ï¼š

```rust
/// ç›´æ¥æ‰©å±•ç°æœ‰ NodeExecutionErrorï¼Œé¿å…æ–°çš„é”™è¯¯å±‚æ¬¡
impl From<SubNodeExecutionError> for NodeExecutionError {
    fn from(error: SubNodeExecutionError) -> Self {
        // ç›´æ¥æ˜ å°„åˆ°ç°æœ‰é”™è¯¯ç±»å‹ï¼Œä¿æŒé”™è¯¯å¤„ç†çš„ä¸€è‡´æ€§
        match error {
            SubNodeExecutionError::LLMExecution(msg) =>
                NodeExecutionError::ExternalServiceError { service: msg },
            SubNodeExecutionError::MemoryOperation(msg) =>
                NodeExecutionError::ExternalServiceError { service: msg },
            SubNodeExecutionError::ToolExecution(msg) =>
                NodeExecutionError::ExternalServiceError { service: msg },
            SubNodeExecutionError::SubNodeNotFound(msg) =>
                NodeExecutionError::ConnectionError(msg),
            SubNodeExecutionError::InvalidConfiguration(msg) =>
                NodeExecutionError::ConfigurationError(msg),
        }
    }
}

/// å†…éƒ¨æ‰§è¡Œé”™è¯¯ç±»å‹ï¼ˆä¸å¯¹å¤–æš´éœ²ï¼‰
#[derive(Debug, thiserror::Error)]
enum SubNodeExecutionError {
    #[error("LLM execution failed: {0}")]
    LLMExecution(String),

    #[error("Memory operation failed: {0}")]
    MemoryOperation(String),

    #[error("Tool execution failed: {0}")]
    ToolExecution(String),

    #[error("Sub Node not found: {0}")]
    SubNodeNotFound(String),

    #[error("Invalid configuration: {0}")]
    InvalidConfiguration(String),
}
```
```

## 10. æ€§èƒ½ä¼˜åŒ–å’Œç›‘æ§

### 10.1 æ€§èƒ½ä¼˜åŒ–æœºåˆ¶

#### ç»“æœç¼“å­˜ç³»ç»Ÿ

```rust
/// Sub Node Provider ç»“æœç¼“å­˜
pub struct SubNodeResultCache {
    cache: Arc<tokio::sync::RwLock<ahash::HashMap<String, CachedResult>>>,
    ttl: std::time::Duration,
    max_size: usize,
}

#[derive(Debug, Clone)]
struct CachedResult {
    result: JsonValue,
    timestamp: std::time::Instant,
    size_bytes: usize,
}

impl SubNodeResultCache {
    pub async fn get(&self, key: &str) -> Option<JsonValue> {
        let cache = self.cache.read().await;
        if let Some(cached) = cache.get(key) {
            if cached.timestamp.elapsed() < self.ttl {
                return Some(cached.result.clone());
            }
        }
        None
    }

    pub async fn put(&self, key: String, result: JsonValue) {
        let mut cache = self.cache.write().await;

        // æ£€æŸ¥å¤§å°é™åˆ¶
        if cache.len() >= self.max_size {
            // æ¸…ç†æœ€æ—§çš„æ¡ç›®
            let oldest_key = cache.keys().next().cloned();
            if let Some(key) = oldest_key {
                cache.remove(&key);
            }
        }

        cache.insert(key, CachedResult {
            result,
            timestamp: std::time::Instant::now(),
            size_bytes: serde_json::to_vec(&result).unwrap_or_default().len(),
        });
    }
}
```

#### è¿æ¥æ± ç®¡ç†

```rust
/// LLM è¿æ¥æ± 
pub struct LLMConnectionPool {
    pool: Arc<tokio::sync::Mutex<Vec<rig::providers::deepseek::Client>>>,
    max_connections: usize,
}

impl LLMConnectionPool {
    pub async fn get_client(&self, api_key: &str) -> rig::providers::deepseek::Client {
        let mut pool = self.pool.lock().await;

        if let Some(client) = pool.pop() {
            client
        } else {
            rig::providers::deepseek::Client::new(api_key)
        }
    }

    pub async fn return_client(&self, client: rig::providers::deepseek::Client) {
        let mut pool = self.pool.lock().await;
        if pool.len() < self.max_connections {
            pool.push(client);
        }
    }
}
```

### 10.2 ç›‘æ§å’Œè°ƒè¯•å·¥å…·

#### æ‰§è¡ŒæŒ‡æ ‡æ”¶é›†

```rust
/// Cluster Node æ‰§è¡ŒæŒ‡æ ‡
#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]
pub struct ClusterExecutionMetrics {
    pub execution_id: String,
    pub start_time: chrono::DateTime<chrono::FixedOffset>,
    pub end_time: Option<chrono::DateTime<chrono::FixedOffset>>,
    pub subnode_count: usize,
    pub llm_calls: u32,
    pub memory_operations: u32,
    pub tool_calls: u32,
    pub total_duration_ms: u64,
    pub success: bool,
    pub error_message: Option<String>,
}

impl ClusterExecutionMetrics {
    pub fn new(execution_id: String) -> Self {
        Self {
            execution_id,
            start_time: fusion_common::time::now_offset(),
            end_time: None,
            subnode_count: 0,
            llm_calls: 0,
            memory_operations: 0,
            tool_calls: 0,
            total_duration_ms: 0,
            success: true,
            error_message: None,
        }
    }

    pub fn finalize(&mut self, success: bool, error_message: Option<String>) {
        self.end_time = Some(fusion_common::time::now_offset());
        self.success = success;
        self.error_message = error_message;
        if let Some(end_time) = self.end_time {
            self.total_duration_ms = (end_time - self.start_time).num_milliseconds() as u64;
        }
    }
}
```

#### è°ƒè¯•å™¨

```rust
/// Cluster Node è°ƒè¯•å™¨
pub struct ClusterNodeDebugger {
    execution_history: Arc<tokio::sync::RwLock<Vec<ClusterExecutionMetrics>>>,
}

impl ClusterNodeDebugger {
    pub async fn record_execution(&self, metrics: ClusterExecutionMetrics) {
        let mut history = self.execution_history.write().await;
        history.push(metrics);

        // ä¿æŒå†å²è®°å½•å¤§å°åœ¨åˆç†èŒƒå›´å†…
        if history.len() > 1000 {
            history.drain(0..100);
        }
    }

    pub async fn get_slow_executions(&self, threshold_ms: u64) -> Vec<ClusterExecutionMetrics> {
        let history = self.execution_history.read().await;
        history.iter()
            .filter(|m| m.total_duration_ms > threshold_ms && m.success)
            .cloned()
            .collect()
    }
}
```

### 10.3 æ€§èƒ½åˆ†æ

```rust
/// æ€§èƒ½åˆ†æå™¨
pub struct ClusterNodeProfiler {
    metrics_collector: Arc<ClusterNodeDebugger>,
    cache: Arc<SubNodeResultCache>,
}

impl ClusterNodeProfiler {
    pub fn new() -> Self {
        Self {
            metrics_collector: Arc::new(ClusterNodeDebugger::new()),
            cache: Arc::new(SubNodeResultCache::new(
                std::time::Duration::from_secs(3600), // 1 hour TTL
                1000, // max 1000 cached results
            )),
        }
    }

    pub async fn start_profiling(&self, execution_id: String) -> ClusterExecutionMetrics {
        ClusterExecutionMetrics::new(execution_id)
    }

    pub async fn finish_profiling(&self, mut metrics: ClusterExecutionMetrics) {
        metrics.finalize(true, None);
        self.metrics_collector.record_execution(metrics).await;
    }

    pub async fn get_performance_summary(&self) -> JsonValue {
        let history = self.metrics_collector.execution_history.read().await;

        if history.is_empty() {
            return json!({
                "total_executions": 0,
                "avg_duration_ms": 0,
                "success_rate": 0.0,
                "cache_hit_rate": 0.0
            });
        }

        let total_executions = history.len();
        let successful_executions = history.iter().filter(|m| m.success).count();
        let total_duration: u64 = history.iter().map(|m| m.total_duration_ms).sum();
        let avg_duration = total_duration / total_executions as u64;
        let success_rate = successful_executions as f64 / total_executions as f64;

        json!({
            "total_executions": total_executions,
            "avg_duration_ms": avg_duration,
            "success_rate": success_rate,
            "cache_size": self.cache.cache.read().await.len()
        })
    }
}
```

## 11. æµ‹è¯•ç­–ç•¥

### 11.1 å•å…ƒæµ‹è¯•

```rust
#[cfg(test)]
mod tests {
    use super::*;
    use hetumind_core::workflow::MockNodeExecutionContext;

    #[tokio::test]
    async fn test_deepseek_provider_creation() {
        let config = crate::llm::deepseek_node::deepseek_v1::DeepSeekNodeConfig::default();
        let deepseek_v1 = crate::llm::deepseek_node::deepseek_v1::DeepseekV1::new().unwrap();
        let provider = DeepSeekLLMSubNodeProvider::from_existing_deepseek_v1(&deepseek_v1, config);

        assert_eq!(provider.provider_type(), SubNodeProviderType::LLM);
        assert!(provider.get_node_definition().kind.to_string().contains("deepseek"));
    }

    #[tokio::test]
    async fn test_memory_provider_creation() {
        let memory_manager = Arc::new(crate::memory::graph_flow_memory::GraphFlowMemoryManager::new());
        let session_id = "test_session".to_string();
        let definition = create_mock_node_definition();
        let provider = SimpleMemorySubNodeProvider::new(memory_manager, session_id, Arc::new(definition));

        assert_eq!(provider.provider_type(), SubNodeProviderType::Memory);
    }
}
```

### 11.2 é›†æˆæµ‹è¯•

```rust
#[cfg(test)]
mod integration_tests {
    use super::*;
    use hetumind_core::workflow::{Workflow, NodeElement};

    #[tokio::test]
    async fn test_cluster_node_workflow() {
        // åˆ›å»ºæµ‹è¯•å·¥ä½œæµ
        let mut workflow = Workflow::new(WorkflowId::now_v7(), "Test Cluster Workflow");

        // æ·»åŠ  AiAgentV2 Root Node
        let ai_agent_node = NodeElement::new("ai_agent", "ai_agent_v2_refactored");
        workflow.add_node(ai_agent_node.clone());

        // æ·»åŠ  DeepSeek LLM Sub Node
        let deepseek_node = NodeElement::new("deepseek", "deepseek_llm_provider");
        workflow.add_node(deepseek_node.clone());

        // æ·»åŠ  Memory Sub Node
        let memory_node = NodeElement::new("memory", "simple_memory_provider");
        workflow.add_node(memory_node.clone());

        // å»ºç«‹è¿æ¥
        workflow.add_connection(
            "ai_agent",
            &ConnectionKind::AiLM,
            vec![Connection::new("deepseek", ConnectionKind::AiLM)]
        );

        workflow.add_connection(
            "ai_agent",
            &ConnectionKind::AiMemory,
            vec![Connection::new("memory", ConnectionKind::AiMemory)]
        );

        // éªŒè¯å·¥ä½œæµç»“æ„
        assert_eq!(workflow.nodes.len(), 3);
        assert_eq!(workflow.connections.len(), 2);
    }
}
```

## 12. æ€»ç»“

### 12.1 æ¶æ„ä¼˜åŠ¿

1. **æ¸è¿›å¼è¿ç§»**ï¼šä¿æŒå‘åå…¼å®¹ï¼Œå…è®¸å¹³æ»‘å‡çº§
2. **æ¨¡å—åŒ–è®¾è®¡**ï¼šSub Node Providers ç‹¬ç«‹å¼€å‘ã€æµ‹è¯•å’Œéƒ¨ç½²
3. **æ€§èƒ½ä¼˜åŒ–**ï¼šæ”¯æŒå¹¶è¡Œæ‰§è¡Œå’Œç¼“å­˜æœºåˆ¶
4. **å¯è§‚æµ‹æ€§**ï¼šå®Œå–„çš„ç›‘æ§å’Œè°ƒè¯•å·¥å…·
5. **å¯æ‰©å±•æ€§**ï¼šå®¹æ˜“æ·»åŠ æ–°çš„ Sub Node ç±»å‹

### 12.2 å®æ–½ä¼˜åŠ¿

1. **å‘åå…¼å®¹**ï¼šé€šè¿‡ä¿æŒ V1 èŠ‚ç‚¹ç¡®ä¿å…¼å®¹æ€§
2. **æ€§èƒ½ä¼˜åŒ–**ï¼šé€šè¿‡ç¼“å­˜å’Œè¿æ¥æ± å‡è½»å½±å“
3. **æµ‹è¯•è¦†ç›–**ï¼šå…¨é¢çš„å•å…ƒæµ‹è¯•å’Œé›†æˆæµ‹è¯•
4. **è°ƒè¯•æ”¯æŒ**ï¼šæä¾›è¯¦ç»†çš„è°ƒè¯•å·¥å…·å’ŒæŒ‡æ ‡

## 13. è¯¦ç»†å®æ–½è®¡åˆ’

### ç¬¬ 1 é˜¶æ®µï¼šåŸºç¡€è®¾æ–½æ­å»ºï¼ˆWeek 1ï¼‰

#### 13.1 åˆ›å»ºæ ¸å¿ƒ Trait å’Œç±»å‹å®šä¹‰

**å®æ–½æ–‡ä»¶**ï¼š
- åˆ›å»º `hetumind-core/src/workflow/sub_node_provider.rs`
- æ‰©å±• `hetumind-core/src/workflow/node_registry.rs`

**æ ¸å¿ƒä»»åŠ¡**ï¼š
- å®ç° `SubNodeProvider`ã€`LLMSubNodeProvider`ã€`MemorySubNodeProvider`ã€`ToolSubNodeProvider` traits
- å®šä¹‰ `SubNodeProviderType` æšä¸¾
- åˆ›å»ºç»Ÿä¸€çš„ç±»å‹åˆ«åç³»ç»Ÿ
- åˆ›å»º `LLMConfig`ã€`LLMResponse`ã€`MemoryConfig` ç­‰é…ç½®ç»“æ„
- æ‰©å±• NodeRegistry æ”¯æŒ SubNodeProvider æ³¨å†Œ

**éªŒæ”¶æ ‡å‡†**ï¼š
- [ ] æ‰€æœ‰ SubNodeProvider traits ç¼–è¯‘é€šè¿‡
- [ ] NodeRegistry æ”¯æŒ Provider æ³¨å†Œå’ŒæŸ¥è¯¢
- [ ] GraphFlow tasks å¯ä»¥æ­£ç¡®é›†æˆåˆ°å·¥ä½œæµä¸­

#### 13.2 GraphFlow é›†æˆä»»åŠ¡å®ç°

**å®æ–½æ–‡ä»¶**ï¼šåˆ›å»º `hetumind-nodes/src/cluster/ai_agent/graph_flow_tasks.rs`

**æ ¸å¿ƒä»»åŠ¡**ï¼š
- å®ç° `MessagePreparationTask`
- å®ç° `LLMProviderTask`
- å®ç° `MemoryProviderTask`
- å®ç° `ResponsePostProcessTask`
- ç¡®ä¿æ‰€æœ‰ Task æ­£ç¡®å®ç° fusion_ai::graph_flow::Task trait

#### 13.3 NodeRegistry æ‰©å±•

**æ ¸å¿ƒä»»åŠ¡**ï¼š
- ç›´æ¥æ‰©å±•ç°æœ‰ NodeRegistry æ·»åŠ  Provider æ³¨å†Œå’ŒæŸ¥è¯¢æ–¹æ³•
- å®ç°ç»Ÿä¸€çš„ç±»å‹åˆ«åç³»ç»Ÿ
- æ·»åŠ  Provider ç”Ÿå‘½å‘¨æœŸç®¡ç†

### ç¬¬ 2 é˜¶æ®µï¼šé‡æ„ DeepSeek LLMï¼ˆWeek 2ï¼‰

#### 13.4 DeepSeekLLMSubNodeProvider å®ç°

**å®æ–½æ–‡ä»¶**ï¼š
- åˆ›å»º `hetumind-nodes/src/llm/deepseek_node/subnode_provider.rs`
- ä¿®æ”¹ `hetumind-nodes/src/llm/deepseek_node/deepseek_v1.rs`ï¼ˆå¦‚æœéœ€è¦ï¼‰

**æ ¸å¿ƒä»»åŠ¡**ï¼š
- åŸºäºç°æœ‰çš„ `DeepseekV1` åˆ›å»º `DeepSeekLLMSubNodeProvider`
- å¤ç”¨ç°æœ‰çš„ DeepSeek é…ç½®å’Œæ‰§è¡Œé€»è¾‘
- å®ç° `call_llm` æ–¹æ³•ï¼Œé›†æˆ rig-core å®¢æˆ·ç«¯
- ç¡®ä¿é”™è¯¯å¤„ç†å’Œèµ„æºç®¡ç†

**éªŒæ”¶æ ‡å‡†**ï¼š
- [ ] DeepSeekLLMSubNodeProvider æ­£ç¡®åŒ…è£…ç°æœ‰ DeepSeek V1 åŠŸèƒ½
- [ ] æ‰€æœ‰æµ‹è¯•ç”¨ä¾‹é€šè¿‡
- [ ] ä¸ç°æœ‰æ¥å£ä¿æŒå…¼å®¹æ€§

### ç¬¬ 3 é˜¶æ®µï¼šé‡æ„ Memory Nodeï¼ˆWeek 2-3ï¼‰

#### 13.5 SimpleMemorySubNodeProvider å®ç°

**å®æ–½æ–‡ä»¶**ï¼šåˆ›å»º `hetumind-nodes/src/memory/simple_memory_node/subnode_provider.rs`

**æ ¸å¿ƒä»»åŠ¡**ï¼š
- åŸºäºç°æœ‰çš„ `SimpleMemoryV1` åˆ›å»º `SimpleMemorySubNodeProvider`
- é›†æˆ `GraphFlowMemoryManager` è¿›è¡Œæ¶ˆæ¯æŒä¹…åŒ–
- å®ç° `store_messages` å’Œ `retrieve_messages` æ–¹æ³•
- ç¡®ä¿ä¸ rig::message::Message çš„æ ¼å¼è½¬æ¢

**éªŒæ”¶æ ‡å‡†**ï¼š
- [ ] SimpleMemorySubNodeProvider æ­£ç¡®åŒ…è£…ç°æœ‰ Memory åŠŸèƒ½
- [ ] æ¶ˆæ¯æ ¼å¼è½¬æ¢æµ‹è¯•é€šè¿‡
- [ ] æŒä¹…åŒ–æœºåˆ¶æ­£å¸¸å·¥ä½œ

### ç¬¬ 4 é˜¶æ®µï¼šé‡æ„ AiAgentV1ï¼ˆWeek 3ï¼‰

#### 13.6 AiAgentV1Refactored å®ç°

**å®æ–½æ–‡ä»¶**ï¼š
- ä¿®æ”¹ `hetumind-nodes/src/cluster/ai_agent/ai_agent_v1.rs`
- åˆ›å»º `hetumind-nodes/src/cluster/ai_agent/cluster_coordinator.rs`

**æ ¸å¿ƒä»»åŠ¡**ï¼š
- åŸºäºç°æœ‰ AiAgentV1 åˆ›å»º `AiAgentV1Refactored`
- å®ç° Sub Node æ”¶é›†é€»è¾‘
- é›†æˆ GraphFlow æ‰§è¡Œç®¡ç†
- ä¿æŒç°æœ‰é…ç½®å’Œå‚æ•°å¤„ç†

#### 13.7 GraphFlow é›†æˆ

**æ ¸å¿ƒä»»åŠ¡**ï¼š
- å®ç° Cluster Graph æ„å»ºé€»è¾‘
- é›†æˆ FlowRunner æ‰§è¡Œç®¡ç†
- å®ç°é”™è¯¯å¤„ç†å’Œæ¢å¤æœºåˆ¶
- ä¼˜åŒ–æ‰§è¡Œæ€§èƒ½

**éªŒæ”¶æ ‡å‡†**ï¼š
- [ ] AiAgentV1Refactored æ­£ç¡®æ”¶é›†å’Œåè°ƒ Sub Nodes
- [ ] GraphFlow æ‰§è¡Œæµç¨‹æ­£å¸¸å·¥ä½œ
- [ ] é”™è¯¯å¤„ç†æœºåˆ¶å®Œå–„

### ç¬¬ 5 é˜¶æ®µï¼šèŠ‚ç‚¹æ³¨å†Œå’Œé›†æˆï¼ˆWeek 3-4ï¼‰

#### 13.8 æ›´æ–°èŠ‚ç‚¹æ³¨å†Œ

**å®æ–½æ–‡ä»¶**ï¼š
- ä¿®æ”¹ `hetumind-nodes/src/cluster/ai_agent/mod.rs`
- ä¿®æ”¹ `hetumind-nodes/src/llm/deepseek_node/mod.rs`
- ä¿®æ”¹ `hetumind-nodes/src/memory/mod.rs`

**æ ¸å¿ƒä»»åŠ¡**ï¼š
- æ³¨å†Œæ–°çš„ SubNodeProviders
- æ›´æ–°èŠ‚ç‚¹æ³¨å†Œé€»è¾‘
- ç¡®ä¿å‘åå…¼å®¹æ€§

```rust
/// æ³¨å†Œæ‰€æœ‰èŠ‚ç‚¹ï¼ŒåŒ…æ‹¬æ–°çš„ Cluster Node æ¶æ„
pub fn register_cluster_nodes(node_registry: &NodeRegistry) -> Result<(), RegistrationError> {
    // 1. æ³¨å†ŒåŸæœ‰èŠ‚ç‚¹ï¼ˆå‘åå…¼å®¹ï¼‰
    register_original_nodes(node_registry)?;

    // 2. æ³¨å†Œé‡æ„åçš„ AiAgentV2
    register_refactored_ai_agent_v2(node_registry)?;

    // 3. æ³¨å†Œ Sub Node Providers
    register_subnode_providers(node_registry)?;

    Ok(())
}
```

#### 13.9 é›†æˆæµ‹è¯•

**å®æ–½æ–‡ä»¶**ï¼šåˆ›å»º `hetumind-nodes/tests/cluster_node_integration_test.rs`

**æ ¸å¿ƒä»»åŠ¡**ï¼š
- ç«¯åˆ°ç«¯å·¥ä½œæµæµ‹è¯•
- Cluster Node å®Œæ•´åŠŸèƒ½éªŒè¯
- æ€§èƒ½å’Œç¨³å®šæ€§æµ‹è¯•

**éªŒæ”¶æ ‡å‡†**ï¼š
- [ ] å®Œæ•´å·¥ä½œæµæµ‹è¯•é€šè¿‡
- [ ] æ€§èƒ½æŒ‡æ ‡ç¬¦åˆé¢„æœŸ
- [ ] é”™è¯¯åœºæ™¯å¤„ç†æ­£ç¡®

### ç¬¬ 6 é˜¶æ®µï¼šæµ‹è¯•å’Œä¼˜åŒ–ï¼ˆWeek 4-5ï¼‰

#### 13.10 å…¨é¢æµ‹è¯•

**æ ¸å¿ƒä»»åŠ¡**ï¼š
- å•å…ƒæµ‹è¯•è¦†ç›–ç‡ > 90%
- é›†æˆæµ‹è¯•è¦†ç›–ä¸»è¦ä½¿ç”¨åœºæ™¯
- æ€§èƒ½æµ‹è¯•éªŒè¯ä¼˜åŒ–æ•ˆæœ
- å›å½’æµ‹è¯•ç¡®ä¿æ— ç ´åæ€§å˜æ›´

#### 13.11 æ€§èƒ½ä¼˜åŒ–

**æ ¸å¿ƒä»»åŠ¡**ï¼š
- å®ç°ç»“æœç¼“å­˜æœºåˆ¶
- ä¼˜åŒ–è¿æ¥æ± ç®¡ç†
- å¹¶è¡Œæ‰§è¡Œä¼˜åŒ–
- å†…å­˜ä½¿ç”¨ä¼˜åŒ–

#### 13.12 æ–‡æ¡£å’Œéƒ¨ç½²

**æ ¸å¿ƒä»»åŠ¡**ï¼š
- æ›´æ–° API æ–‡æ¡£
- ç¼–å†™è¿ç§»æŒ‡å—
- é…ç½®ç¤ºä¾‹å’Œæœ€ä½³å®è·µ
- éƒ¨ç½²å’Œç›‘æ§é…ç½®

## 14. è¿ç§»ç­–ç•¥

### 14.1 å‘åå…¼å®¹æ€§

1. **V1 èŠ‚ç‚¹ä¿æŒä¸å˜**ï¼šç°æœ‰ AiAgentV1 ç»§ç»­å¯ç”¨
2. **é…ç½®å…¼å®¹**ï¼šæ–°æ¶æ„å¯ä»¥æ¥å— V1 æ ¼å¼çš„é…ç½®
3. **å·¥ä½œæµå…¼å®¹**ï¼šç°æœ‰å·¥ä½œæµæ— éœ€ä¿®æ”¹å³å¯ç»§ç»­è¿è¡Œ

### 14.2 é…ç½®è¿ç§»

```rust
/// é…ç½®è¿ç§»å™¨
pub struct ConfigMigrator;

impl ConfigMigrator {
    /// å°† V1 é…ç½®è¿ç§»åˆ° V2
    pub fn migrate_ai_agent_config_v1_to_v2(v1_config: AiAgentConfigV1) -> ClusterNodeConfig {
        ClusterNodeConfig {
            llm_config: Some(LLMConfig {
                model: "deepseek-chat".to_string(),
                temperature: v1_config.temperature,
                max_tokens: Some(128000),
                ..Default::default()
            }),
            memory_config: v1_config.memory_config.map(|mem| MemoryConfig {
                context_window: mem.context_window,
                max_history: mem.max_history,
                persistence_enabled: mem.persistence_enabled,
            }),
            tools_config: None,
            execution_config: ExecutionConfig {
                timeout_seconds: Some(30),
                max_retries: Some(3),
                parallel_execution: Some(true),
            },
        }
    }
}
```

### 14.3 å·¥ä½œæµè¿ç§»

```rust
/// å·¥ä½œæµè¿ç§»å™¨
pub struct WorkflowMigrator;

impl WorkflowMigrator {
    /// å°†ä½¿ç”¨ V1 çš„å·¥ä½œæµè¿ç§»åˆ° V2
    pub fn migrate_workflow_v1_to_v2(workflow: Workflow) -> Result<Workflow, MigrationError> {
        let mut migrated_workflow = workflow.clone();

        // 1. æ›´æ–° AiAgentV1 èŠ‚ç‚¹ä¸º AiAgentV2
        for node in &mut migrated_workflow.nodes {
            if node.kind == "ai_agent_v1" {
                node.kind = "ai_agent_v2".parse().map_err(|_| MigrationError::InvalidNodeKind)?;
                info!("Migrated ai_agent_v1 node to ai_agent_v2: {}", node.name);
            }
        }

        // 2. ç¡®ä¿æ‰€æœ‰å¿…éœ€çš„ Sub Nodes éƒ½å­˜åœ¨
        self.ensure_required_subnodes(&mut migrated_workflow)?;

        Ok(migrated_workflow)
    }
}
```

## 15. é£é™©è¯„ä¼°å’Œç¼“è§£

### æŠ€æœ¯é£é™©

1. **æ€§èƒ½å½±å“**
   - é£é™©ï¼šGraphFlow å¼•å…¥é¢å¤–å¼€é”€
   - ç¼“è§£ï¼šé€šè¿‡ç¼“å­˜å’Œè¿æ¥æ± ä¼˜åŒ–æ€§èƒ½

2. **å…¼å®¹æ€§ç ´å**
   - é£é™©ï¼šæ–°æ¶æ„å¯èƒ½å½±å“ç°æœ‰åŠŸèƒ½
   - ç¼“è§£ï¼šä¿æŒ V1 èŠ‚ç‚¹ä¸å˜ï¼Œç¡®ä¿å‘åå…¼å®¹

3. **å¤æ‚åº¦å¢åŠ **
   - é£é™©ï¼šæ–°æ¶æ„å¢åŠ å­¦ä¹ å’Œç»´æŠ¤æˆæœ¬
   - ç¼“è§£ï¼šå®Œå–„çš„æ–‡æ¡£å’Œæµ‹è¯•ï¼Œé™ä½å­¦ä¹ æ›²çº¿

### å®æ–½é£é™©

1. **è¿›åº¦å»¶æœŸ**
   - é£é™©ï¼šé‡æ„å·¥ä½œé‡è¶…å‡ºé¢„æœŸ
   - ç¼“è§£ï¼šåˆ†é˜¶æ®µå®æ–½ï¼Œæ¯ä¸ªé˜¶æ®µæœ‰æ˜ç¡®çš„é‡Œç¨‹ç¢‘

2. **æµ‹è¯•è¦†ç›–ä¸è¶³**
   - é£é™©ï¼šæ–°åŠŸèƒ½ç¼ºä¹å……åˆ†æµ‹è¯•
   - ç¼“è§£ï¼šåˆ¶å®šä¸¥æ ¼çš„æµ‹è¯•ç­–ç•¥å’Œè´¨é‡æ ‡å‡†

### ç¼“è§£ç­–ç•¥

1. **æ¸è¿›å¼éƒ¨ç½²**ï¼šå…ˆåœ¨å¼€å‘ç¯å¢ƒéªŒè¯ï¼Œå†é€æ­¥æ¨å¹¿åˆ°ç”Ÿäº§ç¯å¢ƒ
2. **ç°åº¦å‘å¸ƒ**ï¼šé€šè¿‡ç‰¹æ€§å¼€å…³æ§åˆ¶æ–°åŠŸèƒ½çš„å¯ç”¨
3. **å›æ»šæœºåˆ¶**ï¼šç¡®ä¿å¯ä»¥å¿«é€Ÿå›æ»šåˆ°ç¨³å®šç‰ˆæœ¬
4. **ç›‘æ§å‘Šè­¦**ï¼šå»ºç«‹å®Œå–„çš„ç›‘æ§å’Œå‘Šè­¦æœºåˆ¶

## 16. æˆåŠŸæ ‡å‡†

### åŠŸèƒ½æ ‡å‡†

- [ ] æ–°æ¶æ„æ”¯æŒæ‰€æœ‰ç°æœ‰åŠŸèƒ½
- [ ] Sub Node Provider ç³»ç»Ÿæ­£å¸¸å·¥ä½œ
- [ ] GraphFlow é›†æˆæ— ç¼ºé™·
- [ ] å‘åå…¼å®¹æ€§ 100%

### æ€§èƒ½æ ‡å‡†

- [ ] å“åº”æ—¶é—´ä¸è¶…è¿‡ç°æœ‰å®ç°çš„ 110%
- [ ] å†…å­˜ä½¿ç”¨ä¸è¶…è¿‡ç°æœ‰å®ç°çš„ 120%
- [ ] æ”¯æŒå¹¶å‘æ‰§è¡Œï¼Œæ€§èƒ½å¯æ‰©å±•

### è´¨é‡æ ‡å‡†

- [ ] ä»£ç è¦†ç›–ç‡ > 90%
- [ ] æ‰€æœ‰å•å…ƒæµ‹è¯•é€šè¿‡
- [ ] é›†æˆæµ‹è¯•è¦†ç›–ä¸»è¦åœºæ™¯
- [ ] æ— ä¸¥é‡ç¼ºé™·å’Œå®‰å…¨æ€§é—®é¢˜

### æ–‡æ¡£æ ‡å‡†

- [ ] API æ–‡æ¡£å®Œæ•´å‡†ç¡®
- [ ] è¿ç§»æŒ‡å—æ¸…æ™°æ˜“æ‡‚
- [ ] é…ç½®ç¤ºä¾‹ä¸°å¯Œå®ç”¨
- [ ] æ•…éšœæ’é™¤æŒ‡å—å®Œå–„

## 17. åç»­ä¼˜åŒ–

### çŸ­æœŸä¼˜åŒ–ï¼ˆå®æ–½å 1-3 ä¸ªæœˆï¼‰

1. **æ€§èƒ½è°ƒä¼˜**ï¼šåŸºäºå®é™…ä½¿ç”¨æ•°æ®ä¼˜åŒ–æ€§èƒ½ç“¶é¢ˆ
2. **åŠŸèƒ½å®Œå–„**ï¼šæ ¹æ®ç”¨æˆ·åé¦ˆå®Œå–„åŠŸèƒ½å’Œä½“éªŒ
3. **å·¥å…·æ”¯æŒ**ï¼šå¼€å‘è°ƒè¯•å’Œç›‘æ§å·¥å…·

### é•¿æœŸè§„åˆ’ï¼ˆå®æ–½å 3-6 ä¸ªæœˆï¼‰

1. **æ›´å¤š Provider**ï¼šæ”¯æŒæ›´å¤š LLM å’Œ Memory æä¾›å•†
2. **é«˜çº§ç‰¹æ€§**ï¼šå®ç°æ›´å¤æ‚çš„æ‰§è¡Œç­–ç•¥å’Œä¼˜åŒ–
3. **ç”Ÿæ€æ‰©å±•**ï¼šæ”¯æŒç¬¬ä¸‰æ–¹ Provider æ’ä»¶ç³»ç»Ÿ

## 19. éƒ¨ç½²å’Œé…ç½®

### 19.1 ç¯å¢ƒå‡†å¤‡

```bash
# 1. å®‰è£…ä¾èµ–
cargo update

# 2. è¿è¡Œæµ‹è¯•
cargo test -p hetumind-nodes --lib --bins

# 3. æ„å»ºé¡¹ç›®
cargo build -p hetumind-nodes

# 4. è¿è¡Œé›†æˆæµ‹è¯•
cargo test -p hetumind-nodes integration_tests
```

### 19.2 é…ç½®ç¤ºä¾‹

```toml
# app.toml - Cluster Node é…ç½®
[hetumind.nodes.cluster_v2]
# AiAgentV2 é…ç½®
[hetumind.nodes.cluster_v2.ai_agent]
version = "2.0.0"
system_prompt = "You are a helpful AI assistant"
max_iterations = 10
temperature = 0.7
enable_streaming = false

# é»˜è®¤ Sub Node é…ç½®
[hetumind.nodes.cluster_v2.default_subnodes]
# LLM é»˜è®¤é…ç½®
[hetumind.nodes.cluster_v2.default_subnodes.llm]
provider = "deepseek"
model = "deepseek-chat"
max_tokens = 128000
temperature = 0.7

# Memory é»˜è®¤é…ç½®
[hetumind.nodes.cluster_v2.default_subnodes.memory]
provider = "simple_memory"
context_window_length = 5
persistence_enabled = false

# æ‰§è¡Œé…ç½®
[hetumind.nodes.cluster_v2.execution]
timeout_seconds = 30
max_retries = 3
parallel_execution = true
max_concurrent_subnodes = 5
```

### 19.3 ç¯å¢ƒå˜é‡

```bash
# DeepSeek API é…ç½®
export DEEPSEEK_API_KEY="your_deepseek_api_key"

# Cluster Node æ‰§è¡Œé…ç½®
export HETUMIND_CLUSTER_MAX_CONCURRENT=5
export HETUMIND_CLUSTER_TIMEOUT=30
export HETUMIND_CLUSTER_CACHE_TTL=3600

# æ€§èƒ½é…ç½®
export HETUMIND_CLUSTER_ENABLE_PARALLEL=true
export HETUMIND_CLUSTER_MEMORY_POOL_SIZE=100
```

## 20. å®æ–½ä¼˜å…ˆçº§å’Œä¸‹ä¸€æ­¥è¡ŒåŠ¨

### ç¬¬1ä¼˜å…ˆçº§ï¼ˆç«‹å³å®æ–½ï¼‰
1. âœ… ç»Ÿä¸€ç±»å‹å®šä¹‰å’Œæ¥å£è®¾è®¡
2. âœ… ä¿®å¤æŠ€æœ¯å†²çª
3. ğŸ”„ éªŒè¯ GraphFlow æ¥å£å®Œæ•´æ€§
4. ğŸ“‹ åˆ›å»ºæ¦‚å¿µéªŒè¯é¡¹ç›®

### ç¬¬2ä¼˜å…ˆçº§ï¼ˆç¬¬1-2å‘¨ï¼‰
1. å®ç° SubNodeProvider traits
2. æ‰©å±• NodeRegistry æ”¯æŒ Provider
3. åˆ›å»ºç®€å•çš„ DeepSeek Provider
4. åŸºç¡€æµ‹è¯•æ¡†æ¶æ­å»º

### ç¬¬3ä¼˜å…ˆçº§ï¼ˆç¬¬3-4å‘¨ï¼‰
1. å®Œæ•´çš„ Provider å®ç°
2. GraphFlow é›†æˆ
3. AiAgentV1Refactored å®ç°
4. é›†æˆæµ‹è¯•

### ç¬¬4ä¼˜å…ˆçº§ï¼ˆç¬¬5å‘¨ï¼‰
1. æ€§èƒ½ä¼˜åŒ–
2. ç›‘æ§å·¥å…·
3. æ–‡æ¡£å®Œå–„
4. éƒ¨ç½²é…ç½®

### ç«‹å³æ‰§è¡Œçš„ä¸‹ä¸€æ­¥è¡ŒåŠ¨

1. **éªŒè¯ GraphFlow æ¥å£**ï¼š
   ```bash
   cargo test --package fusion-ai --lib graph_flow_integration
   ```

2. **åˆ›å»ºæ¦‚å¿µéªŒè¯**ï¼š
   ```bash
   cargo new --bin cluster_node_poc
   ```

3. **æœ¬å‘¨å†…å®Œæˆ**ï¼š
   - åˆ›å»º SubNodeProvider trait å®šä¹‰æ–‡ä»¶
   - å®ç° NodeRegistry æ‰©å±•
   - åˆ›å»ºç®€å• Provider ç¤ºä¾‹

## 21. ç»“è®º

è¿™ä¸ªé›†æˆæ–¹æ¡ˆä¸º hetumind æä¾›äº†ç°ä»£åŒ–ã€æ¨¡å—åŒ–çš„ Cluster Node æ¶æ„ï¼Œæ—¢ä¿æŒäº†ç°æœ‰çš„åŠŸèƒ½å®Œæ•´æ€§ï¼Œåˆä¸ºæœªæ¥çš„æ‰©å±•å’Œä¼˜åŒ–å¥ å®šäº†åšå®çš„åŸºç¡€ã€‚

å…³é”®æˆåŠŸå› ç´ ï¼š
- **æ¸è¿›å¼å®æ–½**ï¼šåˆ†é˜¶æ®µé™ä½é£é™©
- **å‘åå…¼å®¹**ï¼šç¡®ä¿ç°æœ‰ç³»ç»Ÿç¨³å®š
- **å……åˆ†æµ‹è¯•**ï¼šä¿éšœè´¨é‡å’Œå¯é æ€§
- **æŒç»­ä¼˜åŒ–**ï¼šåŸºäºå®é™…ä½¿ç”¨æŒç»­æ”¹è¿›

è¿™ä¸ªé‡æ„å°†ä¸º hetumind å¹³å°å¸¦æ¥æ›´å¼ºçš„æ¨¡å—åŒ–ã€æ›´é«˜çš„å¯æ‰©å±•æ€§å’Œæ›´å¥½çš„ç»´æŠ¤æ€§ã€‚

## 22. ğŸ“š æŠ€æœ¯å†³ç­–æ€»ç»“

### âœ… æœ€ç»ˆç¡®å®šçš„æŠ€æœ¯æ–¹æ¡ˆ

1. **æ¶æ„è®¾è®¡**ï¼šTrait SubNodeProvider æ–¹æ¡ˆ
2. **ç±»å‹ç³»ç»Ÿ**ï¼šç»Ÿä¸€ä½¿ç”¨ `Arc<dyn Trait>` åŒ…è£…
3. **æ‰©å±•æ–¹å¼**ï¼šç›´æ¥æ‰©å±•ç°æœ‰ NodeRegistry
4. **æ‰§è¡Œæ¡†æ¶**ï¼šfusion-ai::graph_flow
5. **é‡æ„ç­–ç•¥**ï¼šæ¸è¿›å¼é‡æ„ï¼Œä¿æŒå‘åå…¼å®¹
6. **é”™è¯¯å¤„ç†**ï¼šä¸åˆ›å»ºæ–°çš„é”™è¯¯å±‚æ¬¡ï¼Œæ‰©å±•ç°æœ‰ NodeExecutionError
7. **å®æ–½è®¡åˆ’**ï¼šè¯¦ç»†çš„6é˜¶æ®µå®æ–½è®¡åˆ’
8. **ç›‘æ§å·¥å…·**ï¼šå®Œæ•´çš„æ€§èƒ½ç›‘æ§å’Œè°ƒè¯•å·¥å…·

### ğŸ”§ ä¿®å¤çš„æŠ€æœ¯å†²çª

1. **ç±»å‹å®šä¹‰å†²çª**ï¼šç»Ÿä¸€ä½¿ç”¨ `Vec<SubNodeProviderRef>`
2. **NodeRegistry æ‰©å±•æ–¹å¼**ï¼šç›´æ¥æ‰©å±•è€ŒéåŒ…è£…å™¨
3. **é”™è¯¯å¤„ç†ç­–ç•¥**ï¼šé¿å…æ–°çš„é”™è¯¯ç±»å‹å±‚æ¬¡
4. **å®æ–½è®¡åˆ’è¯¦ç»†ç¨‹åº¦**ï¼šé‡‡ç”¨è¯¦ç»†çš„6é˜¶æ®µè®¡åˆ’
5. **ç›‘æ§å·¥å…·å®Œå–„åº¦**ï¼šä½¿ç”¨å®Œæ•´çš„ç›‘æ§å’Œè°ƒè¯•å·¥å…·

### ğŸ“‹ æ–‡æ¡£åˆå¹¶ç»“æœ

- âœ… **ç»Ÿä¸€æŠ€æœ¯è§„èŒƒ**ï¼šæ‰€æœ‰æ–‡æ¡£é‡‡ç”¨ç»Ÿä¸€çš„æŠ€æœ¯å†³ç­–
- âœ… **æ¶ˆé™¤å†²çª**ï¼šæ‰€æœ‰æŠ€æœ¯å†²çªå·²æ ¹æ®æ‚¨çš„é€‰æ‹©è§£å†³
- âœ… **å†…å®¹æ•´åˆ**ï¼šå°†ä¸‰ä»½æ–‡æ¡£çš„ä¼˜åŠ¿å†…å®¹åˆå¹¶åˆ°ä¸€ä»½æ–‡æ¡£
- âœ… **å®æ–½æŒ‡å¯¼**ï¼šæä¾›è¯¦ç»†çš„å®æ–½æ­¥éª¤å’Œæ—¶é—´çº¿
- âœ… **é£é™©ç®¡æ§**ï¼šå®Œæ•´çš„é£é™©è¯„ä¼°å’Œç¼“è§£æªæ–½

---

**æ³¨æ„**ï¼šæœ¬æ–‡æ¡£æ˜¯ Cluster Node æ¶æ„çš„æœ€ç»ˆæŠ€æœ¯è§„èŒƒï¼ŒåŒ…å«äº†æ‰€æœ‰è®¾è®¡å†³ç­–ã€å®æ–½è®¡åˆ’å’Œæœ€ä½³å®è·µã€‚æ‰€æœ‰æŠ€æœ¯å†²çªå·²è§£å†³ï¼Œå¯ä»¥ä½œä¸ºå®æ–½çš„å”¯ä¸€å‚è€ƒæ–‡æ¡£ã€‚
