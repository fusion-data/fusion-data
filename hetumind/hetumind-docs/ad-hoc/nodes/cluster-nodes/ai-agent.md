# hetuflow AI Agent ä¸ LLM Chat Model èŠ‚ç‚¹æŠ€æœ¯å®ç°æ–¹æ¡ˆ

## æ¦‚è¿°

æœ¬æ–‡æ¡£åŸºäºå¯¹ n8n AI Agent ä¸ LLM èŠ‚ç‚¹æ•°æ®æµè½¬æœºåˆ¶çš„åˆ†æï¼Œç»“åˆ hetuflow/hetumind é¡¹ç›®çš„æŠ€æœ¯æ¶æ„ï¼Œæå‡ºäº†å®Œæ•´çš„ AI Agent å’Œ LLM Chat Model èŠ‚ç‚¹æŠ€æœ¯å®ç°æ–¹æ¡ˆã€‚è¯¥æ–¹æ¡ˆå……åˆ†åˆ©ç”¨äº† rig-core çš„ AI èƒ½åŠ›ï¼Œå¹¶ä¸ hetumind ç°æœ‰çš„å·¥ä½œæµå¼•æ“æ·±åº¦é›†æˆã€‚

## æœ€æ–°å®ç°çŠ¶æ€

### âœ… å·²å®Œæˆçš„ä¼˜åŒ–

1. **rig-core æ·±åº¦é›†æˆ**ï¼š
   - åœ¨ `hetumind-nodes/Cargo.toml` ä¸­æ·»åŠ äº† `rig-core` ä¾èµ–
   - AI Agent V1 èŠ‚ç‚¹é›†æˆäº† rig-core Agent å’Œ AgentBuilder
   - LLM Chat Model V1 èŠ‚ç‚¹æ”¯æŒ OpenAI å’Œ Anthropic æ¨¡å‹

2. **API å¯†é’¥ç®¡ç†ä¼˜åŒ–**ï¼š
   - LLM èŠ‚ç‚¹æ–°å¢ `credential_id` å‚æ•°æ”¯æŒ
   - å®ç°äº†ä»å‡­è¯æœåŠ¡è·å– API å¯†é’¥çš„æ¡†æ¶
   - ä¿ç•™ç¯å¢ƒå˜é‡å’Œç›´æ¥é…ç½®çš„å…¼å®¹æ€§

3. **å·¥å…·è°ƒç”¨æœºåˆ¶å®Œå–„**ï¼š
   - åˆ›å»ºäº† `ToolManager` å·¥å…·ç®¡ç†å™¨
   - å®ç°äº†åŠ¨æ€å·¥å…·è½¬æ¢æ¡†æ¶
   - æ”¯æŒ JSON æ ¼å¼çš„å·¥å…·å®šä¹‰

### ğŸ”§ æŠ€æœ¯æ¶æ„æ›´æ–°

## 1. æŠ€æœ¯æ¶æ„è®¾è®¡

### 1.1 æ•´ä½“æ¶æ„æ¦‚è§ˆ

```mermaid
graph TB
    subgraph "AI Agent èŠ‚ç‚¹æ¶æ„"
        A[AI Agent Node] --> B[LLM è¿æ¥ç®¡ç†å™¨]
        A --> C[å·¥å…·ç®¡ç†å™¨]
        A --> D[è®°å¿†ç®¡ç†å™¨]
        A --> E[ä¼šè¯çŠ¶æ€ç®¡ç†]

        B --> F[rig-core Agent]
        C --> G[å·¥å…·æ³¨å†Œè¡¨]
        D --> H[ä¸Šä¸‹æ–‡å­˜å‚¨]
        E --> I[æ¶ˆæ¯å†å²]
    end

    subgraph "LLM èŠ‚ç‚¹æ¶æ„"
        J[LLM Chat Model Node] --> K[æ¨¡å‹æä¾›è€…é€‚é…å™¨]
        K --> L[rig-core Client]
        L --> M[OpenAI/Claude/æœ¬åœ°æ¨¡å‹]
    end

    subgraph "æ‰§è¡Œå¼•æ“é›†æˆ"
        N[hetuflow æ‰§è¡Œå¼•æ“] --> O[NodeExecutable Trait]
        O --> P[EngineRequest/Response]
        P --> Q[å­èŠ‚ç‚¹æ‰§è¡Œåè°ƒ]
    end

    A -.-> J
    F -.-> L
    O -.-> N
```

### 1.2 æ ¸å¿ƒç»„ä»¶è¯´æ˜

#### 1.2.1 ä¾èµ–ç®¡ç†ç­–ç•¥

åŸºäºé¡¹ç›®å®é™…æƒ…å†µï¼Œé‡‡ç”¨ä»¥ä¸‹ä¾èµ–ç®¡ç†ç­–ç•¥ï¼š

- **hetumind-nodes ç›´æ¥ä¾èµ– rig-core**ï¼šåœ¨ `Cargo.toml` ä¸­æ·»åŠ äº† `rig-core = { workspace = true, features = ["derive"] }`
- **API å¯†é’¥ç®¡ç†**ï¼šé€šè¿‡ `credential_svc` æœåŠ¡ç®¡ç†ï¼ŒåŸºäº `CredentialEntity` å®ä½“
- **å·¥å…·èŠ‚ç‚¹æ ‡å‡†åŒ–**ï¼šé‡‡ç”¨ JSON æ ¼å¼ä½œä¸ºè¾“å…¥/è¾“å‡ºæ ¼å¼ï¼Œä½¿ç”¨ `ExecutionDataMap` æ•°æ®ç±»å‹

#### 1.2.2 è¿æ¥ç±»å‹è®¾è®¡

åŸºäº n8n çš„è®¾è®¡ï¼Œæˆ‘ä»¬å®šä¹‰ä»¥ä¸‹è¿æ¥ç±»å‹ï¼š

- å¤ç”¨åœ¨ `hetumind/hetumind-core/src/workflow/connection.rs` ä¸­å®šä¹‰çš„ `ConnectionKind` enumç±»å‹

#### 1.2.3 EngineRequest/Response æ¶æ„

```rust
// åœ¨ hetumind-core/src/workflow/engine_request.rs ä¸­
use serde::{Serialize, Deserialize};
use uuid::Uuid;
use ahash::HashMap;

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct EngineRequest<T = HashMap<String, JsonValue>> {
    /// éœ€è¦æ‰§è¡Œçš„åŠ¨ä½œåˆ—è¡¨
    pub actions: Vec<EngineAction>,
    /// è¯·æ±‚å…ƒæ•°æ®
    pub metadata: T,
    /// è¯·æ±‚ID
    pub request_id: Uuid,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum EngineAction {
    /// æ‰§è¡ŒèŠ‚ç‚¹åŠ¨ä½œ
    ExecuteNode(ExecuteNodeAction),
    /// è·å–è¿æ¥æ•°æ®åŠ¨ä½œ
    GetConnectionData(GetConnectionDataAction),
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ExecuteNodeAction {
    /// ç›®æ ‡èŠ‚ç‚¹åç§°
    pub node_name: String,
    /// è¾“å…¥æ•°æ®
    pub input: JsonValue,
    /// è¿æ¥ç±»å‹
    pub connection_type: ConnectionKind,
    /// åŠ¨ä½œID
    pub action_id: Uuid,
    /// åŠ¨ä½œå…ƒæ•°æ®
    pub metadata: HashMap<String, JsonValue>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct EngineResponse<T = HashMap<String, JsonValue>> {
    /// åŠ¨ä½œå“åº”åˆ—è¡¨
    pub action_responses: Vec<EngineResult>,
    /// å“åº”å…ƒæ•°æ®
    pub metadata: T,
    /// å“åº”IDï¼ˆå¯¹åº”è¯·æ±‚IDï¼‰
    pub response_id: Uuid,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct EngineResult {
    /// å¯¹åº”çš„åŠ¨ä½œ
    pub action: EngineAction,
    /// æ‰§è¡Œç»“æœæ•°æ®
    pub data: ExecutionDataMap,
    /// æ‰§è¡ŒçŠ¶æ€
    pub status: ExecutionStatus,
}
```

## 2. AI Agent èŠ‚ç‚¹å®ç°

### 2.1 èŠ‚ç‚¹å®šä¹‰å’Œæ³¨å†Œ

```rust
// åœ¨ hetumind-nodes/src/core/ai_agent/mod.rs ä¸­
use hetumind_core::{
  version::Version,
  workflow::{Node, NodeRegistry, RegistrationError},
};
use std::sync::Arc;

pub mod ai_agent_v1;
pub mod parameters;
pub mod tool_manager;

use ai_agent_v1::AiAgentV1;

pub struct AiAgentNode {
  default_version: Version,
  executors: Vec<NodeExecutor>,
}

impl AiAgentNode {
  pub fn new() -> Result<Self, RegistrationError> {
    let executors: Vec<NodeExecutor> = vec![Arc::new(AiAgentV1::new()?)];
    let default_version = executors.iter().map(|node| node.definition().version.clone()).max().unwrap();
    Ok(Self { default_version, executors })
  }
}

impl Node for AiAgentNode {
  fn default_version(&self) -> &Version {
    &self.default_version
  }

  fn node_executors(&self) -> &[NodeExecutor] {
    &self.executors
  }

  fn kind(&self) -> NodeKind {
    self.executors[0].definition().kind.clone()
  }
}

pub fn register_nodes(node_registry: &NodeRegistry) -> Result<(), RegistrationError> {
  let ai_agent_node = Arc::new(AiAgentNode::new()?);
  node_registry.register_node(ai_agent_node)?;
  Ok(())
}
```

### 2.2 AI Agent V1 å®ç°ï¼ˆå·²ä¼˜åŒ–ï¼‰

```rust
// åœ¨ hetumind-nodes/src/core/ai_agent/ai_agent_v1.rs ä¸­
use std::sync::Arc;

use ahash::{HashMap, HashMapExt};
use async_trait::async_trait;
use hetumind_core::{
    types::JsonValue,
    version::Version,
    workflow::{
        ConnectionKind, EngineAction, EngineRequest, EngineResponse, ExecuteNodeAction,
        ExecutionData, ExecutionDataItems, ExecutionDataMap, InputPortConfig, NodeDefinition,
        NodeDefinitionBuilder, NodeExecutable, NodeExecutionContext, NodeExecutionError,
        NodeProperty, NodePropertyKind, OutputPortConfig, RegistrationError, make_execution_data_map,
    },
};
use rig::{
    agent::{Agent, AgentBuilder},
    completion::Prompt,
    tool::Tool,
};
use serde_json::json;
use uuid::Uuid;

use crate::core::ai_agent::parameters::ToolExecutionStatus;
use crate::core::ai_agent::tool_manager::ToolManager;

use super::parameters::{AiAgentConfig, ModelInstance, ToolCallRequest, ToolCallResult};

#[derive(Debug)]
pub struct AiAgentV1 {
    pub definition: Arc<NodeDefinition>,
    tool_manager: Arc<tokio::sync::RwLock<ToolManager>>,
}

impl AiAgentV1 {
    pub fn new() -> Result<Self, RegistrationError> {
        let definition = NodeDefinitionBuilder::new("ai_agent", "1.0.0")
            .description("AI Agent èŠ‚ç‚¹ï¼Œæ”¯æŒå·¥å…·è°ƒç”¨å’Œè®°å¿†åŠŸèƒ½")
            .category("AI")
            .icon("ğŸ¤–")

            // è¾“å…¥ç«¯å£
            .inputs([
              InputPortConfig::builder()
                .kind(ConnectionKind::main)
                .display_name("Main Input")
                .required(true)
                .build(),
              InputPortConfig::builder()
                .kind(ConnectionKind::AiLanguageModel)
                .display_name("Large Language Model")
                .required(true)
                .max_connections(1)
                .build(),
              InputPortConfig::builder()
                .kind(ConnectionKind::AiMemory)
                .display_name("Memory(Vector storage)")
                .required(false)
                .build(),
              InputPortConfig::builder()
                .kind(ConnectionKind::AiTool)
                .display_name("AI Tool")
                .required(false)
                .build(),
            ])

            // è¾“å‡ºç«¯å£
            .outputs([
                OutputPortConfig::builder()
                  .kind(ConnectionKind::Main)
                  .display_name("AI å“åº”è¾“å‡º")
                  .build(),
                OutputPortConfig::builder()
                  .kind(ConnectionKind::ToolCalls)
                  .display_name("å·¥å…·è°ƒç”¨è¯·æ±‚")
                  .build(),
                OutputPortConfig::builder()
                  .kind(ConnectionKind::Error)
                  .display_name("é”™è¯¯è¾“å‡º")
                  .build(),
            ])

            // å‚æ•°
            .properties([
                NodeProperty::builder()
                  .name("system_prompt")
                  .kind(NodePropertyKind::String)
                  .display_name("ç³»ç»Ÿæç¤ºè¯")
                  .value("ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„AIåŠ©æ‰‹")
                  .required(false),
                NodeProperty::builder()
                  .name("max_iterations")
                  .kind(NodePropertyKind::Number)
                  .display_name("æœ€å¤§è¿­ä»£æ¬¡æ•°")
                  .value(10)
                  .required(false),
                NodeProperty::builder()
                  .name("temperature")
                  .kind(NodePropertyKind::Number)
                  .display_name("æ¸©åº¦å‚æ•°")
                  .value(0.7)
                  .required(false),
            ])

            .build()?;

        Ok(Self { definition })
    }
}

#[async_trait]
impl NodeExecutable for AiAgentV1 {
    async fn execute(
        &self,
        context: &dyn NodeExecutionContext,
        engine_response: Option<&EngineResponse>,
    ) -> Result<ExecutionDataMap, NodeExecutionError> {
        // 1. è·å–è¾“å…¥æ•°æ®å’Œé…ç½®
        let input_data = context.get_input_data("main")?;
        let config: AiAgentConfig = context.get_parameters()?;

        // 2. å¤„ç†å¼•æ“å“åº”ï¼ˆå·¥å…·è°ƒç”¨ç»“æœï¼‰
        if let Some(response) = engine_response {
            return self.handle_tool_responses(context, response, &config).await;
        }

        // 3. è·å–è¿æ¥çš„ LLM å®ä¾‹
        let llm_instance = self.get_llm_instance(context).await?;

        // 4. è·å–è¿æ¥çš„å·¥å…·
        let tools = self.get_tools(context).await?;

        // 5. åˆ›å»º Agent
        let agent = self.create_agent(llm_instance, tools, &config).await?;

        // 6. æ‰§è¡Œ Agent
        let result = agent.prompt(&input_data.to_string()).await
            .map_err(|e| NodeExecutionError::ExecutionFailed(e.to_string()))?;

        // 7. è§£æå“åº”ï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦å·¥å…·è°ƒç”¨
        if let Some(tool_calls) = self.parse_tool_calls(&result) {
            // è¿”å›å¼•æ“è¯·æ±‚ä»¥æ‰§è¡Œå·¥å…·
            return self.create_engine_request(context, tool_calls, &config).await;
        }

        // 8. è¿”å›æœ€ç»ˆç»“æœ
        Ok(make_execution_data_map! {
            "main" => ExecutionDataItems::Single(json!({
                "response": result,
                "agent_type": "ai_agent_v1",
                "timestamp": chrono::Utc::now().timestamp(),
            }))
        })
    }

    fn get_definition(&self) -> &Arc<NodeDefinition> {
        &self.definition
    }
}

impl AiAgentV1 {
    async fn get_llm_instance(&self, context: &dyn NodeExecutionContext)
        -> Result<rig::agent::Agent, NodeExecutionError> {
        // é€šè¿‡è¿æ¥ç±»å‹è·å– LLM å®ä¾‹
        let connection_data = context.get_connection_data(ConnectionKind::AiLanguageModel, 0)
            .await
            .ok_or_else(|| NodeExecutionError::ConnectionError("No LLM model connected".to_string()))?;

        // è§£æ LLM å®ä¾‹
        // è¿™é‡Œéœ€è¦æ ¹æ®å®é™…çš„ LLM èŠ‚ç‚¹å®ç°æ¥è§£æå®ä¾‹
        self.parse_llm_instance(connection_data)
    }

    async fn get_tools(&self, context: &dyn NodeExecutionContext)
        -> Result<Vec<rig::tool::Tool>, NodeExecutionError> {
        // è·å–æ‰€æœ‰è¿æ¥çš„å·¥å…·
        let tool_connections = context.get_all_connections(ConnectionKind::AiTool)
            .await;

        let mut tools = Vec::new();
        for connection in tool_connections {
            if let Ok(tool) = self.parse_tool_instance(connection) {
                tools.push(tool);
            }
        }

        Ok(tools)
    }

    async fn create_agent(
        &self,
        llm: rig::agent::Agent,
        tools: Vec<rig::tool::Tool>,
        config: &AiAgentConfig,
    ) -> Result<rig::agent::Agent, NodeExecutionError> {
        // ä½¿ç”¨ rig-core çš„ AgentBuilder åˆ›å»º Agent
        let mut agent_builder = AgentBuilder::new(llm)
            .with_system_prompt(&config.system_prompt)
            .with_max_iterations(config.max_iterations);

        if !tools.is_empty() {
            agent_builder = agent_builder.with_tools(tools);
        }

        agent_builder.build()
            .map_err(|e| NodeExecutionError::ConfigurationError(e.to_string()))
    }

    async fn handle_tool_responses(
        &self,
        context: &dyn NodeExecutionContext,
        response: &EngineResponse,
        config: &AiAgentConfig,
    ) -> Result<ExecutionDataMap, NodeExecutionError> {
        // å¤„ç†å·¥å…·æ‰§è¡Œç»“æœï¼Œç»§ç»­å¯¹è¯
        let tool_results: Vec<ToolCallResult> = response.action_responses.iter()
            .filter_map(|ar| self.extract_tool_result(ar))
            .collect();

        // æ„å»ºåŒ…å«å·¥å…·ç»“æœçš„æç¤º
        let prompt = self.build_prompt_with_tool_results(context, tool_results, config).await?;

        // è·å– Agent å¹¶æ‰§è¡Œ
        let llm_instance = self.get_llm_instance(context).await?;
        let tools = self.get_tools(context).await?;
        let agent = self.create_agent(llm_instance, tools, config).await?;

        let final_response = agent.prompt(&prompt).await
            .map_err(|e| NodeExecutionError::ExecutionFailed(e.to_string()))?;

        Ok(make_execution_data_map! {
            "main" => ExecutionDataItems::Single(json!({
                "response": final_response,
                "tool_results": tool_results,
                "agent_type": "ai_agent_v1",
                "timestamp": chrono::Utc::now().timestamp(),
            }))
        })
    }

    async fn create_engine_request(
        &self,
        context: &dyn NodeExecutionContext,
        tool_calls: Vec<ToolCallRequest>,
        config: &AiAgentConfig,
    ) -> Result<ExecutionDataMap, NodeExecutionError> {
        let actions: Vec<EngineAction> = tool_calls.into_iter().map(|tool_call| {
            EngineAction::ExecuteNode(ExecuteNodeAction {
                node_name: tool_call.tool_name,
                input: tool_call.parameters,
                connection_type: ConnectionKind::AiTool,
                action_id: uuid::Uuid::new_v4(),
                metadata: {
                    let mut meta = HashMap::new();
                    meta.insert("tool_call_id".to_string(), json!(tool_call.id));
                    meta.insert("tool_name".to_string(), json!(tool_call.tool_name));
                    meta
                },
            })
        }).collect();

        let engine_request = EngineRequest {
            actions,
            metadata: {
                let mut meta = HashMap::new();
                meta.insert("request_type".to_string(), json!("tool_execution"));
                meta.insert("config".to_string(), json!(config));
                meta
            },
            request_id: uuid::Uuid::new_v4(),
        };

        Ok(make_execution_data_map! {
            "tool_calls" => ExecutionDataItems::Single(json!(engine_request))
        })
    }
}
```

### 2.3 é…ç½®ç»“æ„

```rust
// åœ¨ hetumind-nodes/src/core/ai_agent/parameters.rs ä¸­
use serde::{Deserialize, Serialize};

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AiAgentConfig {
    pub system_prompt: String,
    pub max_iterations: u32,
    pub temperature: f64,
    pub enable_streaming: bool,
    pub memory_config: Option<MemoryConfig>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct MemoryConfig {
    pub max_history: usize,
    pub persistence_enabled: bool,
    pub context_window: usize,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ToolCallRequest {
    pub id: String,
    pub tool_name: String,
    pub parameters: JsonValue,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ToolCallResult {
    pub tool_call_id: String,
    pub tool_name: String,
    pub result: JsonValue,
    pub status: ToolExecutionStatus,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum ToolExecutionStatus {
    Success,
    Error(String),
    Timeout,
}
```

## 3. LLM Chat Model èŠ‚ç‚¹å®ç°

### 3.1 èŠ‚ç‚¹å®šä¹‰

```rust
// åœ¨ hetumind-nodes/src/core/llm_chat_model/mod.rs ä¸­
use std::sync::Arc;
use hetumind_core::workflow::{NodeRegistry, RegistrationError};

pub mod llm_chat_model_v1;
pub mod utils;

pub use llm_chat_model_v1::LlmChatModelV1;

pub fn register_nodes(node_registry: &NodeRegistry) -> Result<(), RegistrationError> {
    let llm_node = Arc::new(LlmChatModelV1::new()?);
    node_registry.register_node(llm_node)?;
    Ok(())
}
```

### 3.2 LLM Chat Model V1 å®ç°

```rust
// åœ¨ hetumind-nodes/src/core/llm_chat_model/llm_chat_model_v1.rs ä¸­
use std::sync::Arc;
use async_trait::async_trait;
use hetumind_core::{
    types::JsonValue,
    workflow::{
        ConnectionKind, ExecutionDataItems, ExecutionDataMap, NodeDefinition,
        NodeDefinitionBuilder, NodeExecutable, NodeExecutionContext, NodeExecutionError,
        NodeProperty, NodePropertyKind, InputPortConfig, OutputPortConfig, RegistrationError,
    },
};
use rig::{
    completion::Prompt,
    providers::{
        openai::{Client as OpenAIClient, GPT_4},
        anthropic::{Client as AnthropicClient, CLAUDE_3_OPUS},
    },
};
use serde_json::json;

#[derive(Debug)]
pub struct LlmChatModelV1 {
    pub definition: Arc<NodeDefinition>,
}

impl LlmChatModelV1 {
    pub fn new() -> Result<Self, RegistrationError> {
        let definition = NodeDefinitionBuilder::new("llm_chat_model", "1.0.0")
            .description("LLM èŠå¤©æ¨¡å‹èŠ‚ç‚¹ï¼Œæ”¯æŒå¤šç§æ¨¡å‹æä¾›è€…")
            .category("AI")
            .icon("ğŸ§ ")

            // è¾“å…¥ç«¯å£
            .inputs([InputPortConfig::builder()
              .name(ConnectionKind::Main)
              .description("èŠå¤©æ¶ˆæ¯è¾“å…¥")
              .required(true)
              .build()])

            // è¾“å‡ºç«¯å£
            .outputs([
                OutputPortConfig::builder()
                  .name(ConnectionKind::Main)
                  .description("æ¨¡å‹å“åº”")
                  .build(),
                OutputPortConfig::builder()
                  .name(ConnectionKind::Error)
                  .description("é”™è¯¯è¾“å‡º")
                  .build(),
            ])

            // å‚æ•°
            .properties([
                NodeProperty::builder()
                  .name("provider")
                  .kind(NodePropertyKind::String)
                  .display_name("æ¨¡å‹æä¾›è€…")
                  .value("openai")
                  .required(true),
                NodeProperty::builder()
                  .name("model")
                  .kind(NodePropertyKind::String)
                  .display_name("æ¨¡å‹åç§°")
                  .value("gpt-4")
                  .required(true),
                NodeProperty::builder()
                  .name("api_key")
                  .kind(NodePropertyKind::String)
                  .display_name("API å¯†é’¥")
                  .required(false),  // å¯ä»¥ä»ç¯å¢ƒå˜é‡è·å–
                NodeProperty::builder()
                  .name("base_url")
                  .kind(NodePropertyKind::String)
                  .display_name("API åŸºç¡€URL")
                  .required(false),
                NodeProperty::builder()
                  .name("max_tokens")
                  .kind(NodePropertyKind::Number)
                  .display_name("æœ€å¤§ä»¤ç‰Œæ•°")
                  .value(2000)
                  .required(false),
                NodeProperty::builder()
                  .name("temperature")
                  .kind(NodePropertyKind::Number)
                  .display_name("æ¸©åº¦å‚æ•°")
                  .value(0.7)
                  .required(false),
                NodeProperty::builder()
                  .name("stream")
                  .kind(NodePropertyKind::Boolean)
                  .display_name("æ˜¯å¦å¯ç”¨æµå¼å“åº”")
                  .value(false)
                  .required(false),
            ])

            .build()?;

        Ok(Self { definition })
    }
}

#[async_trait]
impl NodeExecutable for LlmChatModelV1 {
    async fn execute(
        &self,
        context: &dyn NodeExecutionContext,
        _engine_response: Option<&EngineResponse>,
    ) -> Result<ExecutionDataMap, NodeExecutionError> {
        // 1. è·å–è¾“å…¥æ•°æ®å’Œé…ç½®
        let input_data = context.get_input_data("main")?;
        let config: LlmConfig = context.get_parameters()?;

        // 2. åˆ›å»ºæ¨¡å‹å®¢æˆ·ç«¯
        let model_client = self.create_model_client(&config).await?;

        // 3. æ‰§è¡Œæ¨ç†
        let response = if config.stream {
            self.execute_streaming_inference(&model_client, &input_data, &config).await?
        } else {
            self.execute_standard_inference(&model_client, &input_data, &config).await?
        };

        // 4. æ„å»ºè¾“å‡ºæ•°æ®
        Ok(make_execution_data_map! {
            "main" => ExecutionDataItems::Single(response.clone()),
            "model_instance" => ExecutionDataItems::Single(json!({
                "client": model_client,
                "config": config,
                "capabilities": vec!["chat", "completion", "tools"]
            }))
        })
    }

    fn get_definition(&self) -> &Arc<NodeDefinition> {
        &self.definition
    }
}

impl LlmChatModelV1 {
    async fn create_model_client(&self, config: &LlmConfig)
        -> Result<ModelClient, NodeExecutionError> {
        match config.provider.as_str() {
            "openai" => {
                let api_key = config.api_key.as_ref()
                    .or_else(|| std::env::var("OPENAI_API_KEY").ok())
                    .ok_or_else(|| NodeExecutionError::ConfigurationError(
                        "OpenAI API key not provided".to_string()
                    ))?;

                let client = OpenAIClient::new(&api_key);
                let model = match config.model.as_str() {
                    "gpt-4" => GPT_4,
                    "gpt-3.5-turbo" => rig::providers::openai::GPT_3_5_TURBO,
                    _ => return Err(NodeExecutionError::ConfigurationError(
                        format!("Unsupported OpenAI model: {}", config.model)
                    )),
                };

                Ok(ModelClient::OpenAI(client.model(model)))
            },
            "anthropic" => {
                let api_key = config.api_key.as_ref()
                    .or_else(|| std::env::var("ANTHROPIC_API_KEY").ok())
                    .ok_or_else(|| NodeExecutionError::ConfigurationError(
                        "Anthropic API key not provided".to_string()
                    ))?;

                let client = AnthropicClient::new(&api_key);
                let model = match config.model.as_str() {
                    "claude-3-opus" => CLAUDE_3_OPUS,
                    "claude-3-sonnet" => rig::providers::anthropic::CLAUDE_3_SONNET,
                    _ => return Err(NodeExecutionError::ConfigurationError(
                        format!("Unsupported Anthropic model: {}", config.model)
                    )),
                };

                Ok(ModelClient::Anthropic(client.model(model)))
            },
            _ => Err(NodeExecutionError::ConfigurationError(
                format!("Unsupported provider: {}", config.provider)
            )),
        }
    }

    async fn execute_standard_inference(
        &self,
        client: &ModelClient,
        input_data: &JsonValue,
        config: &LlmConfig,
    ) -> Result<JsonValue, NodeExecutionError> {
        let prompt = input_data.get("prompt")
            .and_then(|v| v.as_str())
            .ok_or_else(|| NodeExecutionError::InvalidInput("No prompt provided".to_string()))?;

        let response = match client {
            ModelClient::OpenAI(openai_model) => {
                openai_model.prompt(prompt).await
                    .map_err(|e| NodeExecutionError::ExecutionFailed(e.to_string()))?
            },
            ModelClient::Anthropic(anthropic_model) => {
                anthropic_model.prompt(prompt).await
                    .map_err(|e| NodeExecutionError::ExecutionFailed(e.to_string()))?
            },
        };

        Ok(json!({
            "response": response,
            "model": config.model,
            "provider": config.provider,
            "usage": {
                "tokens": 0, // TODO: ä»å“åº”ä¸­æå–å®é™…ä½¿ç”¨é‡
                "cost": 0.0
            },
            "timestamp": chrono::Utc::now().timestamp(),
        }))
    }

    async fn execute_streaming_inference(
        &self,
        _client: &ModelClient,
        _input_data: &JsonValue,
        _config: &LlmConfig,
    ) -> Result<JsonValue, NodeExecutionError> {
        // TODO: å®ç°æµå¼æ¨ç†
        Ok(json!({
            "response": "Streaming not implemented yet",
            "streaming": true,
            "timestamp": chrono::Utc::now().timestamp(),
        }))
    }
}

#[derive(Debug)]
pub enum ModelClient {
    OpenAI(rig::providers::openai::Model),
    Anthropic(rig::providers::anthropic::Model),
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct LlmConfig {
    pub provider: String,
    pub model: String,
    pub api_key: Option<String>,
    pub base_url: Option<String>,
    pub max_tokens: u32,
    pub temperature: f64,
    pub stream: bool,
}
```

## 4. å·¥ä½œæµå¼•æ“é›†æˆ

### 4.1 æ‰§è¡Œå¼•æ“å¢å¼º

```rust
// åœ¨ hetumind-core/src/workflow/engine.rs ä¸­æ‰©å±•ç°æœ‰å¼•æ“
#[async_trait]
impl WorkflowEngine for DefaultWorkflowEngine {
    async fn execute_workflow(
        &self,
        trigger_data: (NodeName, ExecutionDataMap),
        context: &ExecutionContext,
    ) -> Result<ExecutionResult, WorkflowExecutionError> {
        // ç°æœ‰çš„å·¥ä½œæµæ‰§è¡Œé€»è¾‘

        // æ–°å¢ï¼šå¤„ç† EngineRequest
        let result = self.execute_with_engine_requests(trigger_data, context).await?;

        Ok(result)
    }

    async fn execute_with_engine_requests(
        &self,
        trigger_data: (NodeName, ExecutionDataMap),
        context: &ExecutionContext,
    ) -> Result<ExecutionResult, WorkflowExecutionError> {
        let mut execution_stack = vec![trigger_data];
        let mut pending_requests = Vec::new();

        while !execution_stack.is_empty() {
            let (node_name, input_data) = execution_stack.pop().unwrap();

            // æ‰§è¡ŒèŠ‚ç‚¹
            let node_result = self.execute_node_internal(&node_name, input_data, context).await?;

            // æ£€æŸ¥æ˜¯å¦è¿”å›äº† EngineRequest
            if let Some(engine_request) = self.extract_engine_request(&node_result) {
                // å¤„ç†å¼•æ“è¯·æ±‚
                let request_result = self.handle_engine_request(engine_request, context).await?;

                // å°†ç»“æœä½œä¸ºè¾“å…¥æ•°æ®é‡æ–°æ‰§è¡ŒåŸèŠ‚ç‚¹
                execution_stack.push((node_name, request_result));
            } else {
                // æ­£å¸¸å¤„ç†æ‰§è¡Œç»“æœ
                self.process_execution_result(node_result, &mut execution_stack).await?;
            }
        }

        Ok(ExecutionResult::success())
    }

    async fn handle_engine_request(
        &self,
        request: EngineRequest,
        context: &ExecutionContext,
    ) -> Result<ExecutionDataMap, WorkflowExecutionError> {
        let mut action_responses = Vec::new();

        for action in request.actions {
            match action {
                EngineAction::ExecuteNode(node_action) => {
                    let result = self.execute_node_action(node_action, context).await?;
                    action_responses.push(EngineResult {
                        action: EngineAction::ExecuteNode(node_action),
                        data: result,
                        status: ExecutionStatus::Completed,
                    });
                },
                EngineAction::GetConnectionData(data_action) => {
                    let result = self.get_connection_data_action(data_action, context).await?;
                    action_responses.push(EngineResult {
                        action: EngineAction::GetConnectionData(data_action),
                        data: result,
                        status: ExecutionStatus::Completed,
                    });
                },
            }
        }

        Ok(ExecutionDataMap::new())
    }
}
```

### 4.2 èŠ‚ç‚¹æ‰§è¡Œä¸Šä¸‹æ–‡å¢å¼º

```rust
// åœ¨ hetumind-core/src/workflow/context.rs ä¸­å¢å¼ºç°æœ‰ä¸Šä¸‹æ–‡
#[async_trait]
pub trait NodeExecutionContext: Send + Sync {
    // ç°æœ‰æ–¹æ³•ä¿æŒä¸å˜

    // æ–°å¢ï¼šè·å–è¿æ¥æ•°æ®çš„æ–¹æ³•
    async fn get_connection_data(
        &self,
        connection_type: ConnectionKind,
        index: usize,
    ) -> Option<JsonValue>;

    async fn get_all_connections(&self, connection_type: ConnectionKind) -> Vec<JsonValue>;

    // æ–°å¢ï¼šæ£€æŸ¥æ˜¯å¦æ˜¯å­èŠ‚ç‚¹æ‰§è¡Œ
    fn is_sub_node_execution(&self) -> bool;

    // æ–°å¢ï¼šè·å–å­èŠ‚ç‚¹æ‰§è¡Œç»“æœ
    fn get_sub_node_results(&self) -> Option<&EngineResponse>;
}
```

## 5. æ•°æ®æµè½¬æœºåˆ¶

### 5.1 æ•°æ®æµè½¬æ¶æ„å›¾

```mermaid
sequenceDiagram
    participant U as ç”¨æˆ·è¾“å…¥
    participant AA as AI Agent èŠ‚ç‚¹
    participant WE as å·¥ä½œæµå¼•æ“
    participant LLM as LLM èŠ‚ç‚¹
    participant TOOL as å·¥å…·èŠ‚ç‚¹
    participant ER as EngineRequest/Response

    U->>AA: è¾“å…¥æŸ¥è¯¢
    AA->>WE: è·å–è¿æ¥çš„ LLM å®ä¾‹
    WE->>LLM: è¯·æ±‚æ¨¡å‹å®ä¾‹
    LLM-->>WE: è¿”å› rig-core Agent
    WE-->>AA: æä¾› LLM å®ä¾‹

    AA->>WE: è·å–è¿æ¥çš„å·¥å…·
    WE->>TOOL: è·å–å·¥å…·å®šä¹‰
    TOOL-->>WE: è¿”å›å·¥å…·å…ƒæ•°æ®
    WE-->>AA: æä¾›å·¥å…·åˆ—è¡¨

    AA->>LLM: æ‰§è¡Œæ¨ç†
    LLM-->>AA: è¿”å›å“åº”ï¼ˆå¯èƒ½åŒ…å«å·¥å…·è°ƒç”¨ï¼‰

    alt éœ€è¦å·¥å…·è°ƒç”¨
        AA->>WE: åˆ›å»º EngineRequest
        WE->>TOOL: æ‰§è¡Œå·¥å…·èŠ‚ç‚¹
        TOOL-->>WE: è¿”å›æ‰§è¡Œç»“æœ
        WE->>ER: æ„å»º EngineResponse
        ER->>AA: ä¼ é€’å·¥å…·ç»“æœ
        AA->>LLM: ç»§ç»­å¯¹è¯ï¼ˆåŒ…å«å·¥å…·ç»“æœï¼‰
        LLM-->>AA: è¿”å›æœ€ç»ˆå“åº”
    end

    AA-->>U: è¿”å›æœ€ç»ˆç­”æ¡ˆ
```

### 5.2 å…³é”®æ•°æ®ç»“æ„

```rust
// å·¥å…·è°ƒç”¨æ¶ˆæ¯æ ¼å¼
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ToolCallMessage {
    pub id: String,
    pub tool_name: String,
    pub arguments: JsonValue,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ToolResultMessage {
    pub tool_call_id: String,
    pub tool_name: String,
    pub result: JsonValue,
    pub status: ToolExecutionStatus,
}

// ä¼šè¯æ¶ˆæ¯å†å²
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ConversationMessage {
    pub role: MessageRole,
    pub content: String,
    pub tool_calls: Option<Vec<ToolCallMessage>>,
    pub tool_results: Option<Vec<ToolResultMessage>>,
    pub timestamp: chrono::DateTime<chrono::Utc>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum MessageRole {
    System,
    User,
    Assistant,
    Tool,
}
```

## 6. æ€§èƒ½ä¼˜åŒ–å’Œæ‰©å±•æ€§

### 6.1 è¿æ¥æ± å’Œç¼“å­˜

```rust
// åœ¨ hetumind-core/src/ai/connection_pool.rs ä¸­
use ahash::HashMap;
use std::sync::Arc;
use tokio::sync::RwLock;

pub struct ModelConnectionPool {
    clients: Arc<RwLock<HashMap<String, ModelClient>>>,
    config: ConnectionPoolConfig,
}

impl ModelConnectionPool {
    pub async fn get_client(&self, provider: &str, model: &str) -> Option<ModelClient> {
        let key = format!("{}:{}", provider, model);
        let clients = self.clients.read().await;
        clients.get(&key).cloned()
    }

    pub async fn create_client(&self, provider: &str, model: &str, config: &LlmConfig)
        -> Result<ModelClient, NodeExecutionError> {
        let client = self.create_model_client(provider, model, config).await?;

        let key = format!("{}:{}", provider, model);
        let mut clients = self.clients.write().await;
        clients.insert(key, client.clone());

        Ok(client)
    }
}
```

### 6.2 æµå¼å¤„ç†æ”¯æŒ

```rust
// åœ¨ hetumind-core/src/ai/streaming.rs ä¸­
use futures::Stream;
use tokio::sync::mpsc;

pub struct StreamingResponse {
    pub content_stream: mpsc::Receiver<String>,
    pub metadata: StreamingMetadata,
}

pub struct StreamingMetadata {
    pub request_id: String,
    pub model: String,
    pub provider: String,
    pub total_tokens: u32,
}

#[async_trait]
pub trait StreamingExecutor: Send + Sync {
    async fn execute_streaming(
        &self,
        prompt: &str,
        config: &LlmConfig,
    ) -> Result<StreamingResponse, NodeExecutionError>;

    fn process_stream<S>(&self, stream: S) -> impl Stream<Item = String> + Send
    where
        S: Stream<Item = String> + Send + 'static;
}
```

## 7. é”™è¯¯å¤„ç†å’Œç›‘æ§

### 7.1 é”™è¯¯å¤„ç†ç­–ç•¥

```rust
// åœ¨ hetumind-core/src/ai/error.rs ä¸­
#[derive(Debug, thiserror::Error)]
pub enum AiExecutionError {
    #[error("Model configuration error: {0}")]
    ConfigurationError(String),

    #[error("API request failed: {0}")]
    ApiError(String),

    #[error("Tool execution failed: {0}")]
    ToolExecutionError(String),

    #[error("Rate limit exceeded")]
    RateLimitExceeded,

    #[error("Insufficient credits")]
    InsufficientCredits,

    #[error("Network error: {0}")]
    NetworkError(String),
}

impl From<AiExecutionError> for NodeExecutionError {
    fn from(err: AiExecutionError) -> Self {
        NodeExecutionError::ExecutionFailed(err.to_string())
    }
}
```

### 7.2 ç›‘æ§æŒ‡æ ‡

```rust
// åœ¨ hetumind-core/src/ai/metrics.rs ä¸­
use std::sync::atomic::{AtomicU64, AtomicF64};

pub struct AiMetrics {
    pub total_requests: AtomicU64,
    pub successful_requests: AtomicU64,
    pub failed_requests: AtomicU64,
    pub average_response_time: AtomicF64,
    pub total_tokens_used: AtomicU64,
    pub total_cost: AtomicF64,
    pub active_connections: AtomicU64,
}

impl AiMetrics {
    pub fn record_request(&self, duration: std::time::Duration, tokens: u32, cost: f64, success: bool) {
        self.total_requests.fetch_add(1, std::sync::atomic::Ordering::Relaxed);

        if success {
            self.successful_requests.fetch_add(1, std::sync::atomic::Ordering::Relaxed);
        } else {
            self.failed_requests.fetch_add(1, std::sync::atomic::Ordering::Relaxed);
        }

        self.total_tokens_used.fetch_add(tokens as u64, std::sync::atomic::Ordering::Relaxed);
        self.total_cost.fetch_add(cost, std::sync::atomic::Ordering::Relaxed);

        // æ›´æ–°å¹³å‡å“åº”æ—¶é—´
        let avg_time = self.average_response_time.load(std::sync::atomic::Ordering::Relaxed);
        let total_requests = self.total_requests.load(std::sync::atomic::Ordering::Relaxed);
        let new_avg = (avg_time * (total_requests - 1) as f64 + duration.as_secs_f64()) / total_requests as f64;
        self.average_response_time.store(new_avg, std::sync::atomic::Ordering::Relaxed);
    }
}
```

## 8. å®ç°çŠ¶æ€ä¸åç»­è®¡åˆ’

### 8.1 å½“å‰å®ç°çŠ¶æ€

#### âœ… å·²å®Œæˆ
- rig-core æ·±åº¦é›†æˆï¼Œæ”¯æŒ OpenAI å’Œ Anthropic æ¨¡å‹
- API å¯†é’¥ç®¡ç†æ¡†æ¶ï¼Œæ”¯æŒå‡­è¯æœåŠ¡é›†æˆ
- å·¥å…·è°ƒç”¨æœºåˆ¶åŸºç¡€æ¡†æ¶ï¼Œæ”¯æŒåŠ¨æ€å·¥å…·è½¬æ¢
- EngineRequest/Response æœºåˆ¶å®Œå–„
- AI Agent å’Œ LLM èŠ‚ç‚¹æ ¸å¿ƒåŠŸèƒ½å®ç°

#### ğŸ”§ æŠ€æœ¯å€ºåŠ¡ä¸å¾…å®Œå–„
- **å·¥å…·è½¬æ¢å®ç°**ï¼š`ToolManager.convert_tool_definition()` éœ€è¦å®ç°å…·ä½“çš„å·¥å…·è½¬æ¢é€»è¾‘
- **å‡­è¯æœåŠ¡é›†æˆ**ï¼šLLM èŠ‚ç‚¹çš„å‡­è¯æœåŠ¡è°ƒç”¨éœ€è¦å®é™…é›†æˆ
- **å·¥å…·è°ƒç”¨è§£æ**ï¼šAI Agent çš„å·¥å…·è°ƒç”¨è§£æéœ€è¦å®ç°å…·ä½“é€»è¾‘
- **é”™è¯¯å¤„ç†å¢å¼º**ï¼šéœ€è¦æ›´å®Œå–„çš„é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶
- **æµå¼å“åº”æ”¯æŒ**ï¼šéœ€è¦å®ç°å®Œæ•´çš„æµå¼æ¨ç†èƒ½åŠ›

### 8.2 æ¶æ„ä¼˜åŠ¿æ€»ç»“

1. **åŸºäºæˆç†Ÿæ¨¡å¼**ï¼šå€Ÿé‰´äº† n8n ä¸­ EngineRequest/Response çš„æˆåŠŸè®¾è®¡ï¼Œç¡®ä¿æ¶æ„å¯é æ€§
2. **rig-core æ·±åº¦é›†æˆ**ï¼šå……åˆ†åˆ©ç”¨ rig-core çš„ AI èƒ½åŠ›ï¼Œæ”¯æŒå¤šç§ LLM æä¾›è€…
3. **æ¨¡å—åŒ–è®¾è®¡**ï¼šå„ç»„ä»¶èŒè´£æ¸…æ™°ï¼Œæ˜“äºæ‰©å±•å’Œç»´æŠ¤
4. **å¼‚æ­¥å¹¶å‘å¤„ç†**ï¼šé‡‡ç”¨ Rust çš„å¼‚æ­¥ç¼–ç¨‹æ¨¡å‹ï¼Œç¡®ä¿é«˜æ€§èƒ½
5. **æ ‡å‡†åŒ–æ¥å£**ï¼šé€šè¿‡ ExecutionDataMap å®ç°ç»Ÿä¸€çš„æ•°æ®æµè½¬

### 8.3 åº”ç”¨åœºæ™¯

è¯¥æ–¹æ¡ˆä¸º hetuflow ç³»ç»Ÿæä¾›äº†å¼ºå¤§çš„ AI å·¥ä½œæµèƒ½åŠ›ï¼Œèƒ½å¤Ÿæ”¯æŒï¼š
- æ™ºèƒ½å®¢æœå’Œå¯¹è¯ç³»ç»Ÿ
- è‡ªåŠ¨åŒ–ä»»åŠ¡å¤„ç†
- æ•°æ®åˆ†æå’ŒæŠ¥å‘Šç”Ÿæˆ
- å¤šå·¥å…·åä½œçš„å¤æ‚å·¥ä½œæµ
- AI é©±åŠ¨çš„ä¸šåŠ¡æµç¨‹è‡ªåŠ¨åŒ–

### 8.4 åç»­ä¼˜åŒ–å»ºè®®

1. **å®Œå–„å·¥å…·ç”Ÿæ€**ï¼šå®ç°æ›´å¤šé¢„å®šä¹‰å·¥å…·èŠ‚ç‚¹
2. **å¢å¼ºç›‘æ§èƒ½åŠ›**ï¼šæ·»åŠ  Agent æ‰§è¡ŒæŒ‡æ ‡å’Œé“¾è·¯è¿½è¸ª
3. **æ€§èƒ½ä¼˜åŒ–**ï¼šå®ç°è¿æ¥æ± å’Œç¼“å­˜æœºåˆ¶
4. **å®‰å…¨åŠ å›º**ï¼šå®Œå–„ API å¯†é’¥ç®¡ç†å’Œæƒé™æ§åˆ¶
5. **ç”¨æˆ·ä½“éªŒ**ï¼šä¼˜åŒ–é…ç½®ç•Œé¢å’Œé”™è¯¯æç¤º

é€šè¿‡æ ‡å‡†åŒ–çš„æ¥å£å’Œçµæ´»çš„é…ç½®ï¼Œå¼€å‘è€…å¯ä»¥å¿«é€Ÿæ„å»ºå„ç§ AI åº”ç”¨ï¼Œä¸º hetuflow ç³»ç»Ÿçš„æ™ºèƒ½åŒ–èƒ½åŠ›æä¾›åšå®åŸºç¡€ã€‚
